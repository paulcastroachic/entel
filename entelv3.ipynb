{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',0)\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# for variable transformation\n",
    "import scipy.stats as stats\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "from scipy.optimize import differential_evolution\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score\n",
    "from scikitplot.estimators import plot_feature_importances\n",
    "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics as metrics\n",
    "from datetime import datetime\n",
    "import re\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_month(df,cols,newcol=False):\n",
    "    for col in cols:\n",
    "        if newcol:\n",
    "            cn = col+'_parsed' \n",
    "        else: cn = col\n",
    "        df[cn] = pd.to_datetime(df[col],format='%Y%m')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "          # Total valores perdidos\n",
    "        mis_val = df.isnull().sum()\n",
    "         # porcentaje de valores perdidos\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "         # crea una tabla con los resultados\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        # renombra columnas\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'valores perdidos', 1 : '% de valores totales'})\n",
    "       # ordena de mayor a menor los porcentaje de valores perdidos\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% de valores totales', ascending=False).round(1)\n",
    "        # te muestra la informacion resumida\n",
    "        print (\"la base tiene \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"estos son \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columnas que tienen valores perdidos.\")\n",
    "         # retorna a un dataframe la informacion\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:/usil/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_202201_202203 = pd.read_csv(file_path+'08_target_202201_202203.csv')#\n",
    "convergente_202201_202204 = pd.read_csv(file_path+'07_convergente_202201_202204.csv')\n",
    "trafico_202201_202204 = pd.read_csv(file_path+'06_trafico_202201_202204.csv')#\n",
    "terminales_202201_202204 = pd.read_csv(file_path+'05_terminales_202201_202204.csv')#\n",
    "roaming_202201_202204 = pd.read_csv(file_path+'04_roaming_202201_202204.csv')\n",
    "perfil_digital_202201_202204 = pd.read_csv(file_path+'03_perfil_digital_202201_202204.csv')#\n",
    "adenda_202201_202204 = pd.read_csv(file_path+'02_adenda_202201_202204.csv')#\n",
    "suscriptora_202201_202204 = pd.read_csv(file_path+'01_suscriptora_202201_202204.csv')#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=target_202201_202203.copy()\n",
    "target['key_cliente']=target.PERIODO.astype(str)+'_'+target.nro_telefono_hash.astype(str)\n",
    "target.drop(['PERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "#target_202201_202203['TARGET']=target_202201_202203['TARGET'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# suscriptora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:00<00:00,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01T00:00:00.000000000\n",
      "2022-02-01T00:00:00.000000000\n",
      "2022-03-01T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-01T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "suscriptora=suscriptora_202201_202204.copy()\n",
    "suscriptora['key_cliente']=suscriptora.NUMPERIODO.astype(str)+'_'+suscriptora.nro_telefono_hash.astype(str)\n",
    "suscriptora['key_documento']=suscriptora.NUMPERIODO.astype(str)+'_'+suscriptora.nro_documento_hash.astype(str)\n",
    "\n",
    "suscriptora=parse_month(suscriptora,['NUMPERIODO'],newcol=True)\n",
    "suscriptora['NUMPERIODO_parsed'] =pd.to_datetime(suscriptora['NUMPERIODO_parsed'] , format='%Y/%m/%d')\n",
    "suscriptora.shape\n",
    "\n",
    "#suscriptora.FECINGRESOCLIENTE.str.slice(0,7).value_counts().sort_index()[0:50]\n",
    "for mes in tqdm(suscriptora.NUMPERIODO_parsed.unique()):\n",
    "    print(mes)\n",
    "    suscriptora.loc[suscriptora['NUMPERIODO_parsed'] == mes,'NUMPERIODO_parsed']=pd.to_datetime(mes)+ relativedelta(months=1) - relativedelta(days=1)\n",
    "    \n",
    "suscriptora['FECINGRESOCLIENTE']=np.where(suscriptora['FECINGRESOCLIENTE'].isin(['0001-01-01 00:00:00'])\n",
    "                                           ,np.nan,suscriptora['FECINGRESOCLIENTE'])\n",
    "suscriptora['FECINGRESOCLIENTE'] = pd.to_datetime(suscriptora['FECINGRESOCLIENTE'] , format='%Y-%m-%d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 45/45 [00:00<00:00, 103.84it/s]\n"
     ]
    }
   ],
   "source": [
    "datetime_fold=suscriptora[suscriptora['FECINGRESOCLIENTE']>pd.to_datetime('2022-04-30 00:00:00')]\n",
    "for mes in tqdm(datetime_fold['FECINGRESOCLIENTE'].unique()):\n",
    "    #print(mes)\n",
    "    suscriptora.loc[suscriptora['FECINGRESOCLIENTE'] == mes,'FECINGRESOCLIENTE']=np.nan\n",
    "\n",
    "suscriptora['tiempo_entel']=(suscriptora.NUMPERIODO_parsed-suscriptora.FECINGRESOCLIENTE).astype('<m8[M]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "suscriptora['aniomes_fic']=suscriptora.FECINGRESOCLIENTE.astype(str).str.slice(0,7)#.value_counts().sort_index()[240:]\n",
    "suscriptora['aniomes_fac']=suscriptora.FECACTIVACIONCONTRATO.astype(str).str.slice(0,7)#.value_counts().sort_index()[240:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMANDO LAS VARIABLES DE FECHAS A MESES\n",
    "#suscriptora.FECACTIVACIONCONTRATO.str.slice(0,7).value_counts().sort_index()[0:50]\n",
    "suscriptora['FECACTIVACIONCONTRATO']=np.where(suscriptora['FECACTIVACIONCONTRATO'].isin(['0001-01-01 00:00:00'])\n",
    "                                                                           ,np.nan,suscriptora['FECACTIVACIONCONTRATO'])\n",
    "suscriptora['FECACTIVACIONCONTRATO'] = pd.to_datetime(suscriptora['FECACTIVACIONCONTRATO'] , format='%Y-%m-%d')\n",
    "\n",
    "suscriptora['tiempo_contrato']=suscriptora.NUMPERIODO_parsed-suscriptora.FECACTIVACIONCONTRATO\n",
    "suscriptora['tiempo_contrato']=suscriptora['tiempo_contrato'].astype('<m8[M]')\n",
    "suscriptora['tiempo_contrato_ingreso']=suscriptora.FECACTIVACIONCONTRATO-suscriptora.FECINGRESOCLIENTE\n",
    "suscriptora['tiempo_contrato_ingreso']=suscriptora['tiempo_contrato_ingreso'].astype('<m8[M]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# nro de clientes por Tipo de adquisión\n",
    "suscriptora['TIPO_ADQ_CALC']=suscriptora.TIPO_ADQ\n",
    "for mes in tqdm(suscriptora['NUMPERIODO'].unique()):\n",
    "    NCLIENTES_TIPO_ADQUI=suscriptora.loc[(suscriptora.NUMPERIODO==mes)].TIPO_ADQ.value_counts()\n",
    "    suscriptora.loc[suscriptora.NUMPERIODO==mes,'TIPO_ADQ_CALC']=suscriptora.loc[suscriptora.NUMPERIODO==mes,'TIPO_ADQ'].map(NCLIENTES_TIPO_ADQUI)\n",
    "suscriptora['TIPO_ADQ_CALC']=suscriptora['TIPO_ADQ_CALC'].astype('int')\n",
    "\n",
    "# nro de clientes por Tipo de adquisión\n",
    "suscriptora['aniomes_fic_cal']=suscriptora.aniomes_fic\n",
    "for mes in tqdm(suscriptora['NUMPERIODO'].unique()):\n",
    "    nro_cli_aniomes_fic=suscriptora.loc[(suscriptora.NUMPERIODO==mes)].aniomes_fic.value_counts()\n",
    "    suscriptora.loc[suscriptora.NUMPERIODO==mes,'aniomes_fic_cal']=suscriptora.loc[suscriptora.NUMPERIODO==mes,'aniomes_fic'].map(nro_cli_aniomes_fic)\n",
    "suscriptora['aniomes_fic_cal']=suscriptora['aniomes_fic_cal'].astype('int')\n",
    "\n",
    "# nro de clientes por Tipo de adquisión\n",
    "suscriptora['aniomes_fac_cal']=suscriptora.aniomes_fac\n",
    "for mes in tqdm(suscriptora['NUMPERIODO'].unique()):\n",
    "    nro_cli_aniomes_fac=suscriptora.loc[(suscriptora.NUMPERIODO==mes)].aniomes_fac.value_counts()\n",
    "    suscriptora.loc[suscriptora.NUMPERIODO==mes,'aniomes_fac_cal']=suscriptora.loc[suscriptora.NUMPERIODO==mes,'aniomes_fac'].map(nro_cli_aniomes_fac)\n",
    "suscriptora['aniomes_fac_cal']=suscriptora['aniomes_fac_cal'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suscrip_features_single(doc,periodo):\n",
    "    q1 = suscriptora.query('nro_documento_hash==\"{}\" and NUMPERIODO=={}'.format(doc,periodo))\n",
    "    return q1\n",
    "\n",
    "usg, usgf = [],[]\n",
    "df1=suscriptora[['NUMPERIODO','nro_documento_hash']].drop_duplicates()\n",
    "for doc,periodo in tqdm(zip(df1['nro_documento_hash'],df1['NUMPERIODO'])):\n",
    "    #print(doc,periodo)\n",
    "    a = get_suscrip_features_single(doc,periodo)\n",
    "    b=a.groupby('TIPO_ADQ')['TIPO_ADQ'].transform('count').rename('tipo_adq_counts')\n",
    "    usg.append(pd.concat([a, b], axis=1))\n",
    "suscriptora=pd.concat(usg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suscriptora.to_csv('suscriptora.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNION DE LAS PRIMERAS TABLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION DE SUSCRIPTORA Y TARGET\n",
    "###########################################################\n",
    "suscriptora.drop(['FECINGRESOCLIENTE','FECACTIVACIONCONTRATO'\n",
    "                              ,'NUMPERIODO_parsed','aniomes_fic','aniomes_fac'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(suscriptora,target,on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(734544, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CALCULO DEL DESERTOR POR TELEFONO EN LA HISTORIA ROLLING\n",
    "desertor_hist= []  \n",
    "hist=df_full_hist[['NUMPERIODO','nro_telefono_hash']]\n",
    "hist_fugas=df_full_hist[(df_full_hist.TARGET==1)][['NUMPERIODO','nro_telefono_hash']] # fugas hist\n",
    "for mes in tqdm(hist['NUMPERIODO'].unique()):\n",
    "    partial=hist[hist['NUMPERIODO']==mes][['NUMPERIODO','nro_telefono_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist_fugas[(hist_fugas['nro_telefono_hash'].isin(partial['nro_telefono_hash'].unique())) \n",
    "            & (hist_fugas['NUMPERIODO']<mes)      \n",
    "                    ]\n",
    "    q1=q.groupby('nro_telefono_hash').count().add_suffix('_nro_salidas').fillna(0)\n",
    "    desertor_hist.append(\n",
    "     partial.merge(q1,on='nro_telefono_hash',how='left')\n",
    "    )\n",
    "desertor_hist = pd.concat(desertor_hist,sort=False).fillna(0)\n",
    "desertor_hist.drop_duplicates(inplace=True)\n",
    "desertor_hist['key_cliente']=desertor_hist.NUMPERIODO.astype(str)+'_'+desertor_hist.nro_telefono_hash.astype(str)\n",
    "# renombramos \n",
    "desertor_hist.rename(columns={'NUMPERIODO_nro_salidas':'renovo_hist'},inplace=True)\n",
    "###############################################3\n",
    "# agregamos a la tabla general\n",
    "desertor_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,desertor_hist,on=['key_cliente'],how='left')\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "suscriptora=pd.merge(suscriptora,df_full_hist[['key_cliente','renovo_hist']],on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:16<00:00,  4.06s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(734544, 48)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREANDO VARIABLES EN FUNCION AL TIP_ADQ\n",
    "suscrip = []\n",
    "for mes in tqdm(suscriptora['NUMPERIODO'].unique()):\n",
    "        partial = suscriptora[suscriptora['NUMPERIODO']==mes][['NUMPERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = suscriptora[(suscriptora['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                       & (suscriptora.NUMPERIODO==mes)]\n",
    "        \n",
    "        q1 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['nro_documento_hash'].count().unstack().add_prefix('count_d_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_entel'].mean().unstack().add_prefix('t_e_prom_d_').reset_index().fillna(0)\n",
    "        q3 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_entel'].min().unstack().add_prefix('t_e_min_d_').reset_index().fillna(0)\n",
    "        q4 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_entel'].max().unstack().add_prefix('t_e_max_d_').reset_index().fillna(0)\n",
    "        q5 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_entel'].std().unstack().add_prefix('t_e_std_d_').reset_index().fillna(0)\n",
    "\n",
    "        q6 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato'].mean().unstack().add_prefix('t_c_prom_d_').reset_index().fillna(0)\n",
    "        q7 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato'].min().unstack().add_prefix('t_c_min_d_').reset_index().fillna(0)\n",
    "        q8 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato'].max().unstack().add_prefix('t_c_max_d_').reset_index().fillna(0)\n",
    "        q9 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato'].std().unstack().add_prefix('t_c_std_d_').reset_index().fillna(0)\n",
    "\n",
    "        q10 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato_ingreso'].mean().unstack().add_prefix('t_c_i_prom_d_').reset_index().fillna(0)\n",
    "        q11 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato_ingreso'].min().unstack().add_prefix('t_c_i_min_d_').reset_index().fillna(0)\n",
    "        q12 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato_ingreso'].max().unstack().add_prefix('t_c_i_max_d_').reset_index().fillna(0)\n",
    "        q13 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato_ingreso'].std().unstack().add_prefix('t_c_i_std_d_').reset_index().fillna(0)\n",
    "        \n",
    "        q14 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['renovo_hist'].mean().unstack().add_prefix('t_r_mean_d_').reset_index().fillna(0)\n",
    "        q15 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['renovo_hist'].min().unstack().add_prefix('t_r_min_d_').reset_index().fillna(0)\n",
    "        q16 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['renovo_hist'].max().unstack().add_prefix('t_r_max_d_').reset_index().fillna(0)\n",
    "        q17 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['renovo_hist'].std().unstack().add_prefix('t_r_std_d_').reset_index().fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "        suscrip.append(partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "                           .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "                       .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "                       .merge(q7,on='nro_documento_hash',how='left').merge(q8,on='nro_documento_hash',how='left')\n",
    "                       .merge(q9,on='nro_documento_hash',how='left').merge(q10,on='nro_documento_hash',how='left')\n",
    "                       .merge(q11,on='nro_documento_hash',how='left').merge(q12,on='nro_documento_hash',how='left')\n",
    "                       .merge(q13,on='nro_documento_hash',how='left').merge(q14,on='nro_documento_hash',how='left')\n",
    "                       .merge(q15,on='nro_documento_hash',how='left').merge(q16,on='nro_documento_hash',how='left')\n",
    "                       .merge(q17,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "suscrip = pd.concat(suscrip,sort=False)\n",
    "suscrip=suscrip.drop_duplicates()\n",
    "suscrip['key_documento']=suscrip.NUMPERIODO.astype(str)+'_'+suscrip.nro_documento_hash.astype(str)\n",
    "#############################################################3\n",
    "# UNION DE SUSCRIP\n",
    "suscrip.drop(['NUMPERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,suscrip,on=['key_documento'],how='left')\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(734544, 72)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREANDO VARIABLES \n",
    "suscrip2 = []\n",
    "suscrip_tmp=suscriptora[['NUMPERIODO','nro_documento_hash','tiempo_entel','tiempo_contrato','tiempo_contrato_ingreso','renovo_hist']]\n",
    "for mes in tqdm(suscrip_tmp['NUMPERIODO'].unique()):\n",
    "        partial = suscrip_tmp[suscrip_tmp['NUMPERIODO']==mes][['NUMPERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = suscrip_tmp[(suscrip_tmp['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                       & (suscrip_tmp.NUMPERIODO==mes)]\n",
    "        \n",
    "        q1 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).count().add_prefix('t_d_count_').reset_index().fillna(0)\n",
    "        q2 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).mean().add_prefix('t_d_prom_').reset_index().fillna(0)\n",
    "        q3 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).max().add_prefix('t_d_max_').reset_index().fillna(0)\n",
    "        q4 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).min().add_prefix('t_d_min_').reset_index().fillna(0)\n",
    "        q5 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).median().add_prefix('t_d_median_').reset_index().fillna(0)\n",
    "        q6 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).std().add_prefix('t_d_std_').reset_index().fillna(0)\n",
    "\n",
    "        suscrip2.append(partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "                        .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "                        .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "suscrip2 = pd.concat(suscrip2,sort=False)\n",
    "suscrip2=suscrip2.drop_duplicates()\n",
    "suscrip2['key_documento']=suscrip2.NUMPERIODO.astype(str)+'_'+suscrip2.nro_documento_hash.astype(str)\n",
    "#############################################################3\n",
    "# UNION DE SUSCRIP\n",
    "suscrip2.drop(['NUMPERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,suscrip2,on=['key_documento'],how='left')\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(734544, 74)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUGAS POR TIPO DE ADQUIZ - y telefono\n",
    "fugas = []\n",
    "for mes in tqdm(df_full_hist['NUMPERIODO'].unique()):\n",
    "        partial = df_full_hist[df_full_hist['NUMPERIODO']==mes][['key_cliente']] # verano de enero\n",
    "        q = df_full_hist[(df_full_hist['key_cliente'].isin(partial['key_cliente'].unique()))]\n",
    "        q1 = q.groupby(['key_cliente','TIPO_ADQ'])['renovo_hist'].sum().unstack().add_prefix('cont_fugas_').reset_index().fillna(0)\n",
    "        fugas.append(partial.merge(q1,on='key_cliente',how='left')\n",
    "                        )\n",
    "fugas = pd.concat(fugas,sort=False)\n",
    "fugas=fugas.drop_duplicates()\n",
    "df_full_hist=pd.merge(df_full_hist,fugas,on=['key_cliente'],how='left')\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tabla de digital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nro_telefono_hash: Código de identificación de la línea.\n",
    "#### GRUPO: Categoria de aplicaciones usadas por la línea.\n",
    "#### SCORECAT: Clasificación de que tan digital es el cliente categorizados por niveles.\n",
    "#### PERIODO: Periodo de cierre de mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfil_digital=perfil_digital_202201_202204.copy()\n",
    "perfil_digital['key_cliente']=perfil_digital.PERIODO.astype(str)+'_'+perfil_digital.nro_telefono_hash.astype(str)\n",
    "perfil_digital=pd.merge(perfil_digital,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')\n",
    "perfil_digital['key_documento']=perfil_digital.PERIODO.astype(str)+'_'+perfil_digital.nro_documento_hash.astype(str)\n",
    "\n",
    "dic = {'bajo': 1, 'medio': 2,'alto': 3, 'muy alto': 4}\n",
    "perfil_digital['scortCat_val']= perfil_digital['SCORECAT']\n",
    "perfil_digital['scortCat_val']=perfil_digital['SCORECAT'].map(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_digital_features_single(doc,periodo):\n",
    "    q1 = perfil_digital.query('nro_documento_hash==\"{}\" and PERIODO=={}'.format(doc,periodo))\n",
    "    return q1\n",
    "\n",
    "usg, usgf = [],[]\n",
    "df1=perfil_digital[['PERIODO','nro_documento_hash']].drop_duplicates()\n",
    "for doc,periodo in tqdm(zip(df1['nro_documento_hash'],df1['PERIODO'])):\n",
    "    #print(doc,periodo)\n",
    "    a = get_digital_features_single(doc,periodo)\n",
    "    b=a.groupby('SCORECAT')['SCORECAT'].transform('count').rename('scorecat_counts')\n",
    "    usg.append(pd.concat([a, b], axis=1))\n",
    "perfil_digital=pd.concat(usg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perfil_digital.to_csv('perfil_digital.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# nro de clientes por Tipo de categoria\n",
    "perfil_digital['SCORECAT_CALC']=perfil_digital.SCORECAT\n",
    "for mes in tqdm(perfil_digital['PERIODO'].unique()):\n",
    "    NCLIENTES_TIPO_CATE=perfil_digital.loc[(perfil_digital.PERIODO==mes)].SCORECAT.value_counts()\n",
    "    perfil_digital.loc[perfil_digital.PERIODO==mes,'SCORECAT_CALC']=perfil_digital.loc[perfil_digital.PERIODO==mes,'SCORECAT'].map(NCLIENTES_TIPO_CATE)\n",
    "perfil_digital['SCORECAT_CALC']=perfil_digital['SCORECAT_CALC'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMAION DE LA VARIABLE CADENA DE DATOS A COLUMNAS\n",
    "perfil_digital=pd.concat([perfil_digital,perfil_digital.GRUPO.str.get_dummies(sep = \"|\")], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMANDO TODAS LAS CATEGORIAS USADAS\n",
    "grupo_total=('grupo_1','grupo_2','grupo_3','grupo_4','grupo_5','grupo_6','grupo_7','grupo_8','grupo_9','grupo_10','grupo_11')\n",
    "perfil_digital['grupo_total']=perfil_digital.loc[:,grupo_total].sum(axis=1) # \n",
    "#perfil_digital.drop(['grupo_1','grupo_2','grupo_3','grupo_4','grupo_5','grupo_6','grupo_7','grupo_8','grupo_9','grupo_10','grupo_11'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# por telefono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfil_digital=pd.merge(perfil_digital,df_full_hist[['key_cliente','renovo_hist']],on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo telefono\n",
    "digital = []\n",
    "for mes in tqdm(perfil_digital['PERIODO'].unique()):\n",
    "        partial = perfil_digital[perfil_digital['PERIODO']==mes][['key_cliente']] # verano de enero\n",
    "        q = perfil_digital[(perfil_digital['key_cliente'].isin(partial['key_cliente'].unique()))]\n",
    "        q1 = q.groupby(['key_cliente','SCORECAT'])['grupo_total'].sum().unstack().add_prefix('sum_digi_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['key_cliente','SCORECAT'])['renovo_hist'].sum().unstack().add_prefix('sum_rc_').reset_index().fillna(0)\n",
    "\n",
    "        digital.append(partial.merge(q1,on='key_cliente',how='left').merge(q1,on='key_cliente',how='left')\n",
    "                        )\n",
    "digital = pd.concat(digital,sort=False)\n",
    "digital=digital.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# SCORT DE CATEGORIAS USADAS ROLLING POR TIPO DE USUARIO TELEFONO\n",
    "# el max , min y prom de clasificacion que obtuvo en su historia\n",
    "scortCat_hist= []  \n",
    "hist=perfil_digital[['PERIODO','nro_telefono_hash','scortCat_val','grupo_total','renovo_hist']]\n",
    "for mes in tqdm(hist['PERIODO'].unique()):\n",
    "    partial=hist[hist['PERIODO']==mes][['PERIODO','nro_telefono_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_telefono_hash'].isin(partial['nro_telefono_hash'].unique())) \n",
    "            & (hist['PERIODO']<=mes)\n",
    "                    ]\n",
    "    \n",
    "    q1=q.drop('PERIODO',axis=1).groupby('nro_telefono_hash').max().add_suffix('_nro_max').fillna(0)\n",
    "    q2=q.drop('PERIODO',axis=1).groupby('nro_telefono_hash').min().add_suffix('_nro_min').fillna(0)\n",
    "    q3=q.drop('PERIODO',axis=1).groupby('nro_telefono_hash').mean().add_suffix('_nro_prom').fillna(0)\n",
    "    q4=q.drop('PERIODO',axis=1).groupby('nro_telefono_hash').sum().add_suffix('_nro_sum').fillna(0)\n",
    "\n",
    "    \n",
    "    scortCat_hist.append(\n",
    "     partial.merge(q1,on='nro_telefono_hash',how='left').merge(q2,on='nro_telefono_hash',how='left')\n",
    "        .merge(q3,on='nro_telefono_hash',how='left').merge(q4,on='nro_telefono_hash',how='left')\n",
    "    )\n",
    "scortCat_hist = pd.concat(scortCat_hist,sort=False).fillna(0)\n",
    "scortCat_hist.drop_duplicates(inplace=True)\n",
    "scortCat_hist['key_cliente']=scortCat_hist.PERIODO.astype(str)+'_'+scortCat_hist.nro_telefono_hash.astype(str)\n",
    "#desertor_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:20<00:00,  5.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "digital_cli = []\n",
    "for mes in tqdm(perfil_digital['PERIODO'].unique()):\n",
    "        partial = perfil_digital[perfil_digital['PERIODO']==mes][['PERIODO','nro_telefono_hash']] # verano de enero\n",
    "        q = perfil_digital[(perfil_digital['nro_telefono_hash'].isin(partial['nro_telefono_hash'].unique()))\n",
    "                           & (perfil_digital.PERIODO<=mes)]\n",
    "        \n",
    "        q1 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].sum().unstack().add_prefix('sum_digi_cli_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].mean().unstack().add_prefix('mean_digi_cli_').reset_index().fillna(0)\n",
    "        q3 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].min().unstack().add_prefix('min_digi_cli_').reset_index().fillna(0)\n",
    "        q4 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].max().unstack().add_prefix('max_digi_cli_').reset_index().fillna(0)\n",
    "        q5 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].std().unstack().add_prefix('std_digi_cli_').reset_index().fillna(0)\n",
    "        \n",
    "        q6 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].sum().unstack().add_prefix('sum_digi_rc_').reset_index().fillna(0)\n",
    "        q7 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].mean().unstack().add_prefix('mean_digi_rc_').reset_index().fillna(0)\n",
    "        q8 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].min().unstack().add_prefix('min_digi_rc_').reset_index().fillna(0)\n",
    "        q9 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].max().unstack().add_prefix('max_digi_rc_').reset_index().fillna(0)\n",
    "        q10 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].std().unstack().add_prefix('std_digi_rc_').reset_index().fillna(0)\n",
    "        digital_cli.append(partial.merge(q1,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q2,on='nro_telefono_hash',how='left').merge(q3,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q4,on='nro_telefono_hash',how='left').merge(q5,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q6,on='nro_telefono_hash',how='left').merge(q7,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q8,on='nro_telefono_hash',how='left').merge(q9,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q10,on='nro_telefono_hash',how='left')\n",
    "                        )\n",
    "digital_cli = pd.concat(digital_cli,sort=False)\n",
    "digital_cli=digital_cli.drop_duplicates()\n",
    "digital_cli['key_cliente']=digital_cli.PERIODO.astype(str)+'_'+digital_cli.nro_telefono_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:23<00:00,  5.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "digital_doc = []\n",
    "for mes in tqdm(perfil_digital['PERIODO'].unique()):\n",
    "        partial = perfil_digital[perfil_digital['PERIODO']==mes][['PERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = perfil_digital[(perfil_digital['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                           & (perfil_digital.PERIODO==mes)]\n",
    "        q_ = perfil_digital[(perfil_digital['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                           & (perfil_digital.PERIODO<=mes)]\n",
    "        \n",
    "        q1 = q.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].sum().unstack().add_prefix('sum_digi_doc_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].mean().unstack().add_prefix('mean_digi_doc_').reset_index().fillna(0)\n",
    "        q3 = q_.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].sum().unstack().add_prefix('sum_digi_doc2_').reset_index().fillna(0)\n",
    "        q4 = q_.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].mean().unstack().add_prefix('mean_digi_doc2_').reset_index().fillna(0)\n",
    "        q5 = q.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].min().unstack().add_prefix('min_digi_doc_').reset_index().fillna(0)\n",
    "        q6 = q_.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].min().unstack().add_prefix('min_digi_doc2_').reset_index().fillna(0)\n",
    "        q7 = q.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].max().unstack().add_prefix('max_digi_doc_').reset_index().fillna(0)\n",
    "        q8 = q_.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].max().unstack().add_prefix('max_digi_doc2_').reset_index().fillna(0)\n",
    "        \n",
    "        q9 = q.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].sum().unstack().add_prefix('sum_digi_rc_').reset_index().fillna(0)\n",
    "        q10 = q.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].mean().unstack().add_prefix('mean_digi_rc_').reset_index().fillna(0)\n",
    "        q11 = q_.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].sum().unstack().add_prefix('sum_digi_rc_').reset_index().fillna(0)\n",
    "        q12 = q_.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].mean().unstack().add_prefix('mean_digi_rc_').reset_index().fillna(0)\n",
    "        q13 = q.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].min().unstack().add_prefix('min_digi_rc_').reset_index().fillna(0)\n",
    "        q14 = q_.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].min().unstack().add_prefix('min_digi_rc_').reset_index().fillna(0)\n",
    "        q15 = q.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].max().unstack().add_prefix('max_digi_rc_').reset_index().fillna(0)\n",
    "        q16 = q_.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].max().unstack().add_prefix('max_digi_rc_').reset_index().fillna(0)\n",
    "\n",
    "        digital_doc.append(partial.merge(q1,on='nro_documento_hash',how='left')\n",
    "                           .merge(q2,on='nro_documento_hash',how='left').merge(q3,on='nro_documento_hash',how='left')\n",
    "                           .merge(q4,on='nro_documento_hash',how='left').merge(q5,on='nro_documento_hash',how='left')\n",
    "                           .merge(q6,on='nro_documento_hash',how='left').merge(q7,on='nro_documento_hash',how='left')\n",
    "                           .merge(q8,on='nro_documento_hash',how='left').merge(q9,on='nro_documento_hash',how='left')\n",
    "                           .merge(q10,on='nro_documento_hash',how='left').merge(q11,on='nro_documento_hash',how='left')\n",
    "                           .merge(q12,on='nro_documento_hash',how='left').merge(q13,on='nro_documento_hash',how='left')\n",
    "                           .merge(q14,on='nro_documento_hash',how='left').merge(q15,on='nro_documento_hash',how='left')\n",
    "                           .merge(q16,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "digital_doc = pd.concat(digital_doc,sort=False)\n",
    "digital_doc=digital_doc.drop_duplicates()\n",
    "digital_doc['key_documento']=digital_doc.PERIODO.astype(str)+'_'+digital_doc.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:11<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scortCat_doc_hist= []  \n",
    "hist=perfil_digital[['PERIODO','nro_documento_hash','scortCat_val','grupo_total','renovo_hist']]\n",
    "for mes in tqdm(hist['PERIODO'].unique()):\n",
    "    partial=hist[hist['PERIODO']==mes][['PERIODO','nro_documento_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['PERIODO']<=mes)  \n",
    "            ]\n",
    "    q_ = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['PERIODO']==mes) \n",
    "                    ]\n",
    "    q1=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_snro_max').fillna(0)\n",
    "    q2=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_snro_min').fillna(0)\n",
    "    q3=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_snro_prom').fillna(0)\n",
    "    q4=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_snro_sum').fillna(0)\n",
    "    q5=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_snro_std').fillna(0)\n",
    "\n",
    "       \n",
    "    q6=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_snro_max_2').fillna(0)\n",
    "    q7=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_snro_min_2').fillna(0)\n",
    "    q8=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_snro_prom_2').fillna(0)\n",
    "    q9=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_snro_sum_2').fillna(0)\n",
    "    q10=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_snro_std_2').fillna(0)\n",
    "\n",
    "\n",
    "    \n",
    "    scortCat_doc_hist.append(\n",
    "     partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "        .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "        .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "        .merge(q7,on='nro_documento_hash',how='left').merge(q8,on='nro_documento_hash',how='left')\n",
    "        .merge(q9,on='nro_documento_hash',how='left').merge(q10,on='nro_documento_hash',how='left')\n",
    "    )\n",
    "scortCat_doc_hist = pd.concat(scortCat_doc_hist,sort=False).fillna(0)\n",
    "scortCat_doc_hist.drop_duplicates(inplace=True)\n",
    "scortCat_doc_hist['key_documento']=scortCat_doc_hist.PERIODO.astype(str)+'_'+scortCat_doc_hist.nro_documento_hash.astype(str)\n",
    "#desertor_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(734544, 243)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNION DE LAS TABLAS CREADAS DE DIGITAL\n",
    "perfil_digital.drop(['PERIODO','nro_telefono_hash','nro_documento_hash','key_documento','GRUPO','SCORECAT'],axis=1,inplace=True)\n",
    "digital_doc.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "digital_cli.drop(['PERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "scortCat_hist.drop(['PERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "scortCat_doc_hist.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "df_full_hist=pd.merge(df_full_hist,perfil_digital,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,digital,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,digital_doc,on=['key_documento'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,digital_cli,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,scortCat_hist,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,scortCat_doc_hist,on=['key_documento'],how='left')\n",
    "\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADENDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VCHMESADENDA: Meses transcurridos desde el inicio de la adenda.\n",
    "##### VCHPENALIDAD: Monto de penalidad vigente a la fecha en caso de desactivación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "adenda=adenda_202201_202204.copy()\n",
    "adenda['key_cliente']=adenda.NUMPERIODO.astype(str)+'_'+adenda.nro_telefono_hash.astype(str)\n",
    "adenda=pd.merge(adenda,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')\n",
    "adenda['key_documento']=adenda.NUMPERIODO.astype(str)+'_'+adenda.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adenda_features_single(key_doc,mes_adenda,penalidad,adenda):\n",
    "    q1 = adenda.query('key_documento==\"{}\" '.format(key_doc))\n",
    "    return (mes_adenda < q1['VCHMESADENDA']).mean(), (penalidad < q1['VCHPENALIDAD']).mean()\n",
    "\n",
    "def get_adenda_features(df,adenda):\n",
    "    # No es la forma mas rápida, pero hace el trabajo\n",
    "    usg, usgf = [],[]\n",
    "    adenda['VCHMESADENDA'] = adenda['VCHMESADENDA'].fillna(-999)\n",
    "    adenda['VCHPENALIDAD'] = adenda['VCHPENALIDAD'].fillna(-999)\n",
    "\n",
    "    for key_doc,mes_adenda,penalidad in tqdm(zip(adenda['key_documento'],adenda['VCHMESADENDA'],adenda['VCHPENALIDAD'])):\n",
    "        #print(key_doc,mes_adenda,penalidad)\n",
    "        a,b = get_adenda_features_single(key_doc,mes_adenda,penalidad,adenda)\n",
    "        usg.append(a)\n",
    "        usgf.append(b)\n",
    "    adenda['rel_use'] = usg\n",
    "    adenda['rel_use_full'] = usgf\n",
    "    \n",
    "    adenda['key_documento_count'] = np.nan\n",
    "    for m in tqdm(adenda['key_documento'].unique()):\n",
    "        adenda['key_documento_count'] = np.where(adenda['key_documento']==m,sum(adenda['key_documento']==m),adenda['key_documento_count'])\n",
    "        \n",
    "    adenda['VCHMESADENDA'] = adenda['VCHMESADENDA'].replace(-999,np.nan)\n",
    "    adenda['VCHPENALIDAD'] = adenda['VCHPENALIDAD'].replace(-999,np.nan)\n",
    "  \n",
    "    agg = {'key_documento' : ['count'],\n",
    "                   'rel_use' : ['mean'],\n",
    "           'rel_use_full' : ['mean']}\n",
    "    \n",
    "    adendap = adenda.groupby('key_documento').agg(agg).reset_index()\n",
    "    adendap.columns = ['key_documento']+[a+'_'+b for a,b in adendap.columns[1:]]\n",
    "\n",
    "    return df.merge(adendap,on='key_documento',how='left')\n",
    "df_full_hist = get_adenda_features(df_full_hist, adenda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adenda1=adenda[adenda.key_documento.isin(['202201_7759b41a7d1df4b2d8f721577abc24a48d95dc5f01fbab5937e9b3f6bfe84fc6'\n",
    "                                  ,'202202_f3d5b2a430a3fd220d08e43875ab977aaacbf4e7b167c9993804e5d5e15caafc'\n",
    "                                         ,'202201_b8627a8bd1b8e4e6f05fa2adb87e51cea0ffe232ae0b117ea6604b89f123fa4a'\n",
    "                                         ,'202201_bcddf613461efae35b7001611e394258b231b88cbdb61c03123634ce48682528'])]\n",
    "get_adenda_features(df_full_hist,adenda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# estadisticas a nivel de documento\n",
    "\n",
    "adenda_doc_hist= []  \n",
    "hist=adenda[['NUMPERIODO','nro_documento_hash','VCHMESADENDA','VCHPENALIDAD']]\n",
    "for mes in tqdm(hist['NUMPERIODO'].unique()):\n",
    "    partial=hist[hist['NUMPERIODO']==mes][['NUMPERIODO','nro_documento_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']<=mes)      \n",
    "                    ]\n",
    "    q_ = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']==mes)\n",
    "             ]\n",
    "    q1=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_nro_max').fillna(0)\n",
    "    q2=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_nro_min').fillna(0)\n",
    "    q3=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_nro_prom').fillna(0)\n",
    "    q4=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_nro_sum').fillna(0)\n",
    "    q5=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_nro_std').fillna(0)\n",
    "\n",
    "\n",
    "    \n",
    "    q6=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_nro_max2').fillna(0)\n",
    "    q7=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_nro_min2').fillna(0)\n",
    "    q8=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_nro_prom2').fillna(0)\n",
    "    q9=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_nro_sum2').fillna(0)\n",
    "    q10=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_nro_std2').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "    adenda_doc_hist.append(\n",
    "     partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "        .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "        .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "        .merge(q7,on='nro_documento_hash',how='left').merge(q8,on='nro_documento_hash',how='left')\n",
    "        .merge(q9,on='nro_documento_hash',how='left').merge(q10,on='nro_documento_hash',how='left')\n",
    "    )\n",
    "adenda_doc_hist = pd.concat(adenda_doc_hist,sort=False).fillna(0)\n",
    "adenda_doc_hist.drop_duplicates(inplace=True)\n",
    "adenda_doc_hist['key_documento']=adenda_doc_hist.NUMPERIODO.astype(str)+'_'+adenda_doc_hist.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION DE LAS TABLAS CREADAS DE adendas\n",
    "adenda.drop(['NUMPERIODO','nro_telefono_hash','nro_documento_hash','key_documento'],axis=1,inplace=True)\n",
    "#adenda_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "adenda_doc_hist.drop(['NUMPERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "\n",
    "#df_full_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,adenda,on=['key_cliente'],how='left')\n",
    "#df_full_hist=pd.merge(df_full_hist,adenda_hist,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,adenda_doc_hist,on=['key_documento'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734544, 265)\n"
     ]
    }
   ],
   "source": [
    "print(df_full_hist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAFICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafico=trafico_202201_202204.copy()\n",
    "trafico['key_cliente']=trafico.NUMPERIODO.astype(str)+'_'+trafico.nro_telefono_hash.astype(str)\n",
    "trafico=pd.merge(trafico,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMANDO TODAS los traficos \n",
    "trafico_subtotal=('trafico_app_1','trafico_app_2' ,'trafico_app_7','trafico_app_8'\n",
    "             ,'trafico_app_3','trafico_app_4','trafico_app_5','trafico_app_6','trafico_app_9')\n",
    "trafico['trafico_subtotal']=trafico.loc[:,trafico_subtotal].sum(axis=1) \n",
    "trafico['peso_traf']=trafico['trafico_subtotal']/trafico['trafico_total']\n",
    "\n",
    "mins_flujo=('mins_flujo_1','mins_flujo_2')\n",
    "trafico['mins_flujo_total']=trafico.loc[:,mins_flujo].sum(axis=1) \n",
    "trafico['peso_flujo1']=trafico['mins_flujo_1']/trafico['mins_flujo_total']\n",
    "trafico['peso_flujo2']=trafico['mins_flujo_2']/trafico['mins_flujo_total']\n",
    "\n",
    "#trafico.drop(['trafico_app_1','trafico_app_2' ,'trafico_app_7','trafico_app_8'\n",
    "             #,'trafico_app_3','trafico_app_4','trafico_app_5','trafico_app_6','trafico_app_9'\n",
    "            # ,'mins_flujo_1','mins_flujo_2'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "traf_doc_hist= []  \n",
    "hist=trafico[['NUMPERIODO','nro_documento_hash','trafico_subtotal','mins_flujo_total','trafico_total']]\n",
    "for mes in tqdm(hist['NUMPERIODO'].unique()):\n",
    "    partial=hist[hist['NUMPERIODO']==mes][['NUMPERIODO','nro_documento_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']<=mes)      \n",
    "                    ]\n",
    "    q_ = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']==mes)      \n",
    "                    ]\n",
    "    q1=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_tr_prom').fillna(0)\n",
    "    q2=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_tr_sum').fillna(0)\n",
    "    q3=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_tr_min').fillna(0)\n",
    "    q4=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_tr_max').fillna(0)\n",
    "    q5=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_tr_std').fillna(0)\n",
    "\n",
    "    q6=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_tr_prom2').fillna(0)\n",
    "    q7=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_tr_sum2').fillna(0)\n",
    "    q8=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_tr_min2').fillna(0)\n",
    "    q9=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_tr_max2').fillna(0)\n",
    "    q10=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_tr_std2').fillna(0)\n",
    "\n",
    "\n",
    "    traf_doc_hist.append(\n",
    "     partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "        .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "        .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "        .merge(q7,on='nro_documento_hash',how='left').merge(q8,on='nro_documento_hash',how='left')\n",
    "        .merge(q9,on='nro_documento_hash',how='left').merge(q10,on='nro_documento_hash',how='left')\n",
    "    )\n",
    "traf_doc_hist = pd.concat(traf_doc_hist,sort=False).fillna(0)\n",
    "traf_doc_hist.drop_duplicates(inplace=True)\n",
    "traf_doc_hist['key_documento']=traf_doc_hist.NUMPERIODO.astype(str)+'_'+traf_doc_hist.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:12<00:00,  3.21s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "traf_tel_hist= []  \n",
    "hist=trafico[['NUMPERIODO','nro_telefono_hash','trafico_subtotal','mins_flujo_total','trafico_total']]\n",
    "for mes in tqdm(hist['NUMPERIODO'].unique()):\n",
    "    partial=hist[hist['NUMPERIODO']==mes][['NUMPERIODO','nro_telefono_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_telefono_hash'].isin(partial['nro_telefono_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']<=mes)      \n",
    "                    ]\n",
    "    q1=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').mean().add_suffix('_tel_tr_prom').fillna(0)\n",
    "    q2=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').sum().add_suffix('_tel_tr_sum').fillna(0)\n",
    "    q3=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').min().add_suffix('_tel_tr_min').fillna(0)\n",
    "    q4=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').max().add_suffix('_tel_tr_max').fillna(0)\n",
    "    q5=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').std().add_suffix('_tel_tr_std').fillna(0)\n",
    "\n",
    "\n",
    "    traf_tel_hist.append(\n",
    "     partial.merge(q1,on='nro_telefono_hash',how='left').merge(q2,on='nro_telefono_hash',how='left')\n",
    "        .merge(q3,on='nro_telefono_hash',how='left').merge(q4,on='nro_telefono_hash',how='left')\n",
    "        .merge(q5,on='nro_telefono_hash',how='left')\n",
    "    )\n",
    "traf_tel_hist = pd.concat(traf_tel_hist,sort=False).fillna(0)\n",
    "traf_tel_hist.drop_duplicates(inplace=True)\n",
    "traf_tel_hist['key_cliente']=traf_tel_hist.NUMPERIODO.astype(str)+'_'+traf_tel_hist.nro_telefono_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "traf_tel_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "traf_doc_hist.drop(['NUMPERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "\n",
    "df_full_hist=pd.merge(df_full_hist,traf_tel_hist,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,traf_doc_hist,on=['key_documento'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734544, 327)\n"
     ]
    }
   ],
   "source": [
    "trafico.drop(['NUMPERIODO','nro_telefono_hash','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,trafico,on=['key_cliente'],how='left')\n",
    "print(df_full_hist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# terminales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_low_classes(lst,thresh,string):\n",
    "    c2r = lst.value_counts()[lst.value_counts()<thresh].index\n",
    "    return lst.replace({x:string for x in c2r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminales=terminales_202201_202204.copy()\n",
    "terminales['key_cliente']=terminales.PERIODO.astype(str)+'_'+terminales.nro_telefono_hash.astype(str)\n",
    "terminales=parse_month(terminales,['PERIODO'],newcol=True)\n",
    "#terminales.drop(['PERIODO','nro_telefono_hash'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 87.24it/s]\n"
     ]
    }
   ],
   "source": [
    "terminales['LANZAMIENTO'] = pd.to_datetime(terminales['LANZAMIENTO'] , format='%Y-%m-%d')\n",
    "datetime_fold=terminales[terminales['LANZAMIENTO']>pd.to_datetime('2022-04-30 00:00:00')]\n",
    "for mes in tqdm(datetime_fold['LANZAMIENTO'].unique()):\n",
    "    #print(mes)\n",
    "    terminales.loc[terminales['LANZAMIENTO'] == mes,'LANZAMIENTO']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.27it/s]\n"
     ]
    }
   ],
   "source": [
    "terminales['aniomes_lanz']=terminales.LANZAMIENTO.astype(str).str.slice(0,4)\n",
    "# nro de clientes por Tipo de adquisión\n",
    "terminales['aniomes_lanz_cal']=terminales.aniomes_lanz\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    nro_cli_aniomes_lanz=terminales.loc[(terminales.PERIODO==mes)].aniomes_lanz.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'aniomes_lanz_cal']=terminales.loc[terminales.PERIODO==mes,'aniomes_lanz'].map(nro_cli_aniomes_lanz)\n",
    "terminales['aniomes_lanz_cal']=terminales['aniomes_lanz_cal'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformacion para optener la variable de tiempo de lanzamiento\n",
    "terminales['lanzamiento_equipo']=(terminales.PERIODO_parsed-terminales.LANZAMIENTO).astype('<m8[M]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transfomracion de las variables categoricas\n",
    "terminales['MARCA'] = replace_low_classes(terminales['MARCA'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "terminales['MODELO'] = replace_low_classes(terminales['MODELO'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "terminales['NUEVA_GAMMA'] = replace_low_classes(terminales['NUEVA_GAMMA'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "terminales['OS'] = replace_low_classes(terminales['OS'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "terminales['DEVICE_TYPE'] = replace_low_classes(terminales['DEVICE_TYPE'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "\n",
    "terminales['MARCA']=terminales['MARCA'].astype('category')\n",
    "terminales['MODELO']=terminales['MODELO'].astype('category')\n",
    "terminales['NUEVA_GAMMA']=terminales['NUEVA_GAMMA'].astype('category')\n",
    "terminales['OS']=terminales['OS'].astype('category')\n",
    "terminales['DEVICE_TYPE']=terminales['DEVICE_TYPE'].astype('category')\n",
    "\n",
    "terminales['MARCA']= terminales['MARCA'].cat.codes\n",
    "terminales['MODELO']= terminales['MODELO'].cat.codes\n",
    "terminales['NUEVA_GAMMA']= terminales['NUEVA_GAMMA'].cat.codes\n",
    "terminales['OS']= terminales['OS'].cat.codes\n",
    "terminales['DEVICE_TYPE']= terminales['DEVICE_TYPE'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 21.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 21.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 19.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 21.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 19.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# nro de clientes por Tipo de scort de categoria\n",
    "terminales['MARCA_CALC']=terminales.MARCA\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_MARCA=terminales.loc[(terminales.PERIODO==mes)].MARCA.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'MARCA_CALC']=terminales.loc[terminales.PERIODO==mes,'MARCA'].map(NCLIENTES_MARCA)\n",
    "terminales['MARCA_CALC']=terminales['MARCA_CALC'].astype('int')\n",
    "\n",
    "terminales['MODELO_CALC']=terminales.MODELO\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_MODELO=terminales.loc[(terminales.PERIODO==mes)].MODELO.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'MODELO_CALC']=terminales.loc[terminales.PERIODO==mes,'MODELO'].map(NCLIENTES_MODELO)\n",
    "terminales['MODELO_CALC']=terminales['MODELO_CALC'].astype('int')\n",
    "\n",
    "terminales['NUEVA_GAMMA_CALC']=terminales.NUEVA_GAMMA\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_NUEVA_GAMMA=terminales.loc[(terminales.PERIODO==mes)].NUEVA_GAMMA.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'NUEVA_GAMMA_CALC']=terminales.loc[terminales.PERIODO==mes,'NUEVA_GAMMA'].map(NCLIENTES_NUEVA_GAMMA)\n",
    "terminales['NUEVA_GAMMA_CALC']=terminales['NUEVA_GAMMA_CALC'].astype('int')\n",
    "\n",
    "terminales['OS_CALC']=terminales.OS\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_OS=terminales.loc[(terminales.PERIODO==mes)].OS.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'OS_CALC']=terminales.loc[terminales.PERIODO==mes,'OS'].map(NCLIENTES_OS)\n",
    "terminales['OS_CALC']=terminales['OS_CALC'].astype('int')\n",
    "\n",
    "terminales['DEVICE_TYPE_CALC']=terminales.DEVICE_TYPE\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_DEVICE_TYPE=terminales.loc[(terminales.PERIODO==mes)].DEVICE_TYPE.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'DEVICE_TYPE_CALC']=terminales.loc[terminales.PERIODO==mes,'DEVICE_TYPE'].map(NCLIENTES_DEVICE_TYPE)\n",
    "terminales['DEVICE_TYPE_CALC']=terminales['DEVICE_TYPE_CALC'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminales=pd.merge(terminales,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_features_single(doc,periodo):\n",
    "    q1 = terminales.query('nro_documento_hash==\"{}\" and PERIODO=={}'.format(doc,periodo))\n",
    "    return q1\n",
    "\n",
    "usg, usgf = [],[]\n",
    "df1=terminales[['PERIODO','nro_documento_hash']].drop_duplicates()\n",
    "for doc,periodo in tqdm(zip(df1['nro_documento_hash'],df1['PERIODO'])):\n",
    "    #print(doc,periodo)\n",
    "    a = get_term_features_single(doc,periodo)\n",
    "    b=a.groupby('MARCA')['MARCA'].transform('count').rename('marca_counts')\n",
    "    c=a.groupby('MODELO')['MODELO'].transform('count').rename('modelo_counts')\n",
    "    d=a.groupby('NUEVA_GAMMA')['NUEVA_GAMMA'].transform('count').rename('ngama_counts')\n",
    "    e=a.groupby('OS')['OS'].transform('count').rename('os_counts')\n",
    "    f=a.groupby('DEVICE_TYPE')['DEVICE_TYPE'].transform('count').rename('device_counts')\n",
    "    usg.append(pd.concat([a, b, c, d, e, f], axis=1))\n",
    "terminales=pd.concat(usg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_term_features_single(doc,periodo):\n",
    "    q1 = terminales.query('nro_documento_hash==\"{}\" and PERIODO=={}'.format(doc,periodo))\n",
    "    return q1\n",
    "\n",
    "usg, usgf = [],[]\n",
    "terminales1=terminales[['PERIODO','nro_documento_hash']].drop_duplicates()\n",
    "for doc,periodo in tqdm(zip(terminales1['nro_documento_hash'],terminales1['PERIODO'])):\n",
    "    #print(doc,periodo)\n",
    "    a = get_term_features_single(doc,periodo)\n",
    "    b=a.groupby('MARCA')['MARCA'].transform('count')\n",
    "    usg.append(pd.concat([a, b], axis=1))\n",
    "c=pd.concat(usg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "terminales_doc_1 = []\n",
    "data_term=terminales[['nro_documento_hash','PERIODO','lanzamiento_equipo']]\n",
    "for mes in tqdm(data_term['PERIODO'].unique()):\n",
    "        partial = data_term[data_term['PERIODO']==mes][['PERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = data_term[(data_term['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                     & (data_term['PERIODO']==mes)]\n",
    "        \n",
    "        q1 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).mean().add_prefix('doc_ter_prom_').reset_index().fillna(0)\n",
    "        q2 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).min().add_prefix('doc_ter_min_').reset_index().fillna(0)\n",
    "        q3 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).max().add_prefix('doc_ter_max_').reset_index().fillna(0)\n",
    "        q4 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).std().add_prefix('doc_ter_std_').reset_index().fillna(0)\n",
    "\n",
    "        \n",
    "\n",
    "        terminales_doc_1.append(partial.merge(q1,on='nro_documento_hash',how='left')\n",
    "                                .merge(q2,on='nro_documento_hash',how='left').merge(q3,on='nro_documento_hash',how='left')\n",
    "                                .merge(q4,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "terminales_doc_1 = pd.concat(terminales_doc_1,sort=False)\n",
    "terminales_doc_1=terminales_doc_1.drop_duplicates()\n",
    "terminales_doc_1['key_documento']=terminales_doc_1.PERIODO.astype(str)+'_'+terminales_doc_1.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "terminales_doc= []  \n",
    "hist=terminales[['PERIODO','nro_documento_hash','MARCA','MODELO','NUEVA_GAMMA','OS','DEVICE_TYPE']]\n",
    "for mes in tqdm(hist['PERIODO'].unique()):\n",
    "    partial=hist[hist['PERIODO']==mes][['PERIODO','nro_documento_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['PERIODO']==mes)      \n",
    "                    ]\n",
    "    q1=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').nunique().add_suffix('_nunique_prod_').fillna(0)\n",
    "    q2=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').count().add_suffix('_count_prod_').fillna(0)\n",
    "\n",
    "\n",
    "    terminales_doc.append(\n",
    "     partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "       \n",
    "    )\n",
    "terminales_doc = pd.concat(terminales_doc,sort=False).fillna(0)\n",
    "terminales_doc.drop_duplicates(inplace=True)\n",
    "terminales_doc['key_documento']=terminales_doc.PERIODO.astype(str)+'_'+terminales_doc.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734544, 339)\n",
      "(734544, 349)\n"
     ]
    }
   ],
   "source": [
    "######################################33\n",
    "terminales.drop(['PERIODO','nro_telefono_hash','LANZAMIENTO','PERIODO_parsed','nro_documento_hash','aniomes_lanz'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,terminales,on=['key_cliente'],how='left')\n",
    "print(df_full_hist.shape)\n",
    "\n",
    "terminales_doc.drop(['PERIODO','nro_documento_hash','MODELO_count_prod_','NUEVA_GAMMA_count_prod_','OS_count_prod_','DEVICE_TYPE_count_prod_'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,terminales_doc,on=['key_documento'],how='left')\n",
    "\n",
    "terminales_doc_1.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,terminales_doc_1,on=['key_documento'],how='left')\n",
    "\n",
    "print(df_full_hist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convergente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergente=convergente_202201_202204.copy()\n",
    "convergente['key_documento']=convergente.PERIODO.astype(str)+'_'+convergente.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiene_prod_total=('TIENE_PROD_1','TIENE_PROD_2','TIENE_PROD_3')\n",
    "convergente['tiene_prod_total']=convergente.loc[:,tiene_prod_total].sum(axis=1) \n",
    "\n",
    "convergente['peso_PROD_1']=convergente['TIENE_PROD_1']/convergente['tiene_prod_total']\n",
    "convergente['peso_PROD_2']=convergente['TIENE_PROD_2']/convergente['tiene_prod_total']\n",
    "convergente['peso_PROD_3']=convergente['TIENE_PROD_3']/convergente['tiene_prod_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergente['GIRO'] = replace_low_classes(convergente['GIRO'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "convergente['SUBGIRO'] = replace_low_classes(convergente['SUBGIRO'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "\n",
    "convergente['GIRO']=convergente['GIRO'].astype('category')\n",
    "convergente['SUBGIRO']=convergente['SUBGIRO'].astype('category')\n",
    "\n",
    "convergente['GIRO']= convergente['GIRO'].cat.codes\n",
    "convergente['SUBGIRO']= convergente['SUBGIRO'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 40.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 52.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# nro de clientes por Tipo de scort de categoria\n",
    "convergente['GIRO_CALC']=convergente.GIRO\n",
    "for mes in tqdm(convergente['PERIODO'].unique()):\n",
    "    NCLIENTES_GIRO=convergente.loc[(convergente.PERIODO==mes)].GIRO.value_counts()\n",
    "    convergente.loc[convergente.PERIODO==mes,'GIRO_CALC']=convergente.loc[convergente.PERIODO==mes,'GIRO'].map(NCLIENTES_GIRO)\n",
    "convergente['GIRO_CALC']=convergente['GIRO_CALC'].astype('int')\n",
    "\n",
    "convergente['SUBGIRO_CALC']=convergente.SUBGIRO\n",
    "for mes in tqdm(convergente['PERIODO'].unique()):\n",
    "    NCLIENTES_SUBGIRO=convergente.loc[(convergente.PERIODO==mes)].SUBGIRO.value_counts()\n",
    "    convergente.loc[convergente.PERIODO==mes,'SUBGIRO_CALC']=convergente.loc[convergente.PERIODO==mes,'SUBGIRO'].map(NCLIENTES_SUBGIRO)\n",
    "convergente['SUBGIRO_CALC']=convergente['SUBGIRO_CALC'].astype('int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "convergente_hist = []\n",
    "data_conv=convergente[['nro_documento_hash','PERIODO','tiene_prod_total']]\n",
    "for mes in tqdm(data_conv['PERIODO'].unique()):\n",
    "        partial = data_conv[data_conv['PERIODO']==mes][['PERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = data_conv[(data_conv['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                     & (data_conv['PERIODO']<=mes)]\n",
    "        q_ = data_conv[(data_conv['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                     & (data_conv['PERIODO']==mes)]\n",
    "        \n",
    "        q1 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).sum().add_prefix('doc_conv_prom_').reset_index().fillna(0)\n",
    "        q2 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).sum().add_prefix('doc_conv_prom2_').reset_index().fillna(0)\n",
    "        q3 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).mean().add_prefix('doc_conv_mean_').reset_index().fillna(0)\n",
    "        q4 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).mean().add_prefix('doc_conv_mean2_').reset_index().fillna(0)\n",
    "        q5 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).min().add_prefix('doc_conv_min_').reset_index().fillna(0)\n",
    "        q6 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).min().add_prefix('doc_conv_min2_').reset_index().fillna(0)\n",
    "        q7 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).max().add_prefix('doc_conv_max_').reset_index().fillna(0)\n",
    "        q8 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).max().add_prefix('doc_conv_max2_').reset_index().fillna(0)\n",
    "        q9 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).std().add_prefix('doc_conv_std_').reset_index().fillna(0)\n",
    "        q10 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).std().add_prefix('doc_conv_std2_').reset_index().fillna(0)\n",
    "        \n",
    "        convergente_hist.append(partial.merge(q1,on='nro_documento_hash',how='left')\n",
    "                                .merge(q2,on='nro_documento_hash',how='left').merge(q3,on='nro_documento_hash',how='left')\n",
    "                                .merge(q4,on='nro_documento_hash',how='left').merge(q5,on='nro_documento_hash',how='left')\n",
    "                                .merge(q6,on='nro_documento_hash',how='left').merge(q7,on='nro_documento_hash',how='left')\n",
    "                                .merge(q8,on='nro_documento_hash',how='left').merge(q9,on='nro_documento_hash',how='left')\n",
    "                                .merge(q10,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "convergente_hist = pd.concat(convergente_hist,sort=False)\n",
    "convergente_hist=convergente_hist.drop_duplicates()\n",
    "convergente_hist['key_documento']=convergente_hist.PERIODO.astype(str)+'_'+convergente_hist.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734544, 370)\n"
     ]
    }
   ],
   "source": [
    "convergente.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,convergente,on=['key_documento'],how='left')\n",
    "\n",
    "convergente_hist.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,convergente_hist,on=['key_documento'],how='left')\n",
    "\n",
    "print(df_full_hist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-70915339d787>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  roaming['key_cliente']=roaming.PERIODO.astype(str)+'_'+roaming.nro_telefono_hash.astype(str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(170028, 8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roaming=roaming_202201_202204.drop_duplicates()\n",
    "roaming['key_cliente']=roaming.PERIODO.astype(str)+'_'+roaming.nro_telefono_hash.astype(str)\n",
    "roaming=pd.merge(roaming,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')\n",
    "roaming['key_documento']=roaming.PERIODO.astype(str)+'_'+roaming.nro_documento_hash.astype(str)\n",
    "roaming.drop(['nro_telefono_hash','key_cliente'],axis=1,inplace=True)\n",
    "roaming.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "roaming['FECHATRAFICO'] = pd.to_datetime(roaming['FECHATRAFICO'] , format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166671, 9)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roaming=roaming[roaming['FECHATRAFICO']>=pd.to_datetime('2022-01-01 00:00:00')]\n",
    "roaming['codmes_fec_traf']=roaming['FECHATRAFICO'].astype(str).str.slice(0,4)+roaming['FECHATRAFICO'].astype(str).str.slice(5,7)\n",
    "roaming.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "roaming['TIPOSERVICIO'] = replace_low_classes(roaming['TIPOSERVICIO'],thresh=1000,string='TIPO2').values # 7 =7 a más"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TIPO1    104503\n",
       "TIPO2     62168\n",
       "Name: TIPOSERVICIO, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roaming.TIPOSERVICIO.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "roaming_hist = []\n",
    "data_roaming=roaming[['key_documento','PERIODO','MINUTOS','GIGAS','MENSAJES']]\n",
    "for mes in tqdm(data_roaming['PERIODO'].unique()):\n",
    "        partial = data_roaming[data_roaming['PERIODO']==mes][['PERIODO','key_documento']] # verano de enero\n",
    "        q = data_roaming[(data_roaming['key_documento'].isin(partial['key_documento'].unique()))\n",
    "                     & (data_roaming['PERIODO']==mes)]\n",
    "        \n",
    "        q1 = q.drop('PERIODO',axis=1).groupby(['key_documento']).sum().add_prefix('key_sum_').reset_index().fillna(0)\n",
    "        q2 = q.drop('PERIODO',axis=1).groupby(['key_documento']).mean().add_prefix('key_prom_').reset_index().fillna(0)\n",
    "        q3 = q.drop('PERIODO',axis=1).groupby(['key_documento']).median().add_prefix('key_md_').reset_index().fillna(0)\n",
    "        roaming_hist.append(partial.merge(q1,on='key_documento',how='left')\n",
    "                                .merge(q2,on='key_documento',how='left').merge(q3,on='key_documento',how='left')\n",
    "                        )\n",
    "roaming_hist = pd.concat(roaming_hist,sort=False)\n",
    "roaming_hist=roaming_hist.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "roaming_hist2 = []\n",
    "for mes in tqdm(roaming['PERIODO'].unique()):\n",
    "        partial = roaming[roaming['PERIODO']==mes][['PERIODO','key_documento']] # verano de enero\n",
    "        q = roaming[(roaming['key_documento'].isin(partial['key_documento'].unique()))\n",
    "                           & (roaming.PERIODO==mes)]\n",
    "        \n",
    "        q1 = q.groupby(['key_documento','TIPOSERVICIO'])['MINUTOS'].sum().unstack().add_prefix('sum_min_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['key_documento','TIPOSERVICIO'])['MINUTOS'].mean().unstack().add_prefix('mean_min_').reset_index().fillna(0)\n",
    "        q3 = q.groupby(['key_documento','TIPOSERVICIO'])['GIGAS'].sum().unstack().add_prefix('sum_gigas_').reset_index().fillna(0)\n",
    "        q4 = q.groupby(['key_documento','TIPOSERVICIO'])['GIGAS'].mean().unstack().add_prefix('mean_gigas_').reset_index().fillna(0)\n",
    "        q5 = q.groupby(['key_documento','TIPOSERVICIO'])['MENSAJES'].sum().unstack().add_prefix('sum_sms_').reset_index().fillna(0)\n",
    "        q6 = q.groupby(['key_documento','TIPOSERVICIO'])['MENSAJES'].mean().unstack().add_prefix('mean_sms_').reset_index().fillna(0)\n",
    "\n",
    "        roaming_hist2.append(partial.merge(q1,on='key_documento',how='left')\n",
    "                           .merge(q2,on='key_documento',how='left').merge(q3,on='key_documento',how='left')\n",
    "                           .merge(q4,on='key_documento',how='left').merge(q5,on='key_documento',how='left')\n",
    "                             .merge(q6,on='key_documento',how='left')\n",
    "                        )\n",
    "roaming_hist2 = pd.concat(roaming_hist2,sort=False)\n",
    "roaming_hist2=roaming_hist2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(734544, 391)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roaming_hist.drop(['PERIODO'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,roaming_hist,on=['key_documento'],how='left')\n",
    "df_full_hist.shape\n",
    "\n",
    "roaming_hist2.drop(['PERIODO'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,roaming_hist2,on=['key_documento'],how='left')\n",
    "df_full_hist.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INGRESO DE VARIABLES AL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_full_hist.to_csv('df_full_hist_2.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_hist=pd.read_csv(file_path+'df_full_hist_2.csv')\n",
    "df_full_hist_2=pd.read_csv(file_path+'df_full_hist.csv')\n",
    "perfil_digital=pd.read_csv(file_path+'perfil_digital.csv')\n",
    "suscriptora=pd.read_csv(file_path+'suscriptora.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(734544, 400)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_full_hist.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "tmp=df_full_hist_2[['key_cliente','marca_counts','modelo_counts','ngama_counts'\n",
    "                                        ,'os_counts','device_counts','rel_use','rel_use_full']]\n",
    "df_full_hist=pd.merge(df_full_hist,tmp,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,perfil_digital[['key_cliente','scorecat_counts']],on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,suscriptora[['key_cliente','tipo_adq_counts']],on=['key_cliente'],how='left')\n",
    "del df_full_hist_2,perfil_digital,suscriptora\n",
    "df_full_hist.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_full_hist.copy()\n",
    "df.drop(['NUMPERIODO','nro_documento_hash', 'nro_telefono_hash', 'key_documento'],axis=1,inplace=True)\n",
    "categorical=[\n",
    "# suscriptora\n",
    "        'TIPO_ADQ'\n",
    "    #terminales\n",
    "        #,'MARCA','MODELO','NUEVA_GAMMA','OS','DEVICE_TYPE'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datos_imputados(variable):\n",
    "    # buscamos variables categoricas y discretas\n",
    "    temp = df.groupby([variable])[variable].count()/np.float64(len(df))\n",
    "    frecuencia_cat = [x for x in temp.loc[temp>0.01].index.values]\n",
    "    \n",
    "    df[variable] = np.where(df[variable].isin(frecuencia_cat), df[variable], 'Raro')\n",
    "\n",
    "for var in categorical:\n",
    "    datos_imputados(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codificar_var_categoricas(var, target):\n",
    "        # tomamos como referencia el target para poder codificar\n",
    "        ordenamos_cat = df.groupby([var])[target].sum().sort_values().index # ordenamos en funcion a peso del target\n",
    "        transf_cat = {k:i for i, k in enumerate(ordenamos_cat, 0)} # transformamos a continuas\n",
    "        \n",
    "        # codificamos el set\n",
    "        df[var] = df[var].map(transf_cat)\n",
    "\n",
    "# codificamos las variables\n",
    "for var in categorical:\n",
    "    codificar_var_categoricas(var, 'TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (547995, 394) Testing (186549, 394)\n"
     ]
    }
   ],
   "source": [
    "df_forw=df.copy()\n",
    "df_forw['clase'] = 0\n",
    "df_forw.loc[df_forw.TARGET.notnull(),'clase']='train' #training\n",
    "df_forw.loc[df_forw.TARGET.isnull(),'clase']='test' #back_test\n",
    "train=df_forw.loc[df_forw.clase=='train'].set_index('key_cliente').drop(['clase','TARGET'],axis=1)\n",
    "test=df_forw.loc[df_forw.clase=='test'].set_index('key_cliente').drop(['clase','TARGET'],axis=1)\n",
    "\n",
    "y_train=df_forw.loc[df_forw.clase=='train'].set_index('key_cliente').TARGET.astype(int)  ## cambiamos el tipo de target\n",
    "y_test=df_forw.loc[df_forw.clase=='test'].set_index('key_cliente').TARGET ## cambiamos el tipo de target\n",
    "# limpiamos nommbres y caracteres extraños de nombres de columnas\n",
    "test = test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_-]+', '', x))\n",
    "train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_-]+', '', x))\n",
    "print('Training',train.shape,'Testing',test.shape) ###(358487, 2630) (396666, 2630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['t_e_std_d_tipo1', 't_e_std_d_tipo2', 't_d_std_tiempo_entel', 'grupo_3',\n",
      "       'grupo_5', 'doc_conv_std2_tiene_prod_total', 'sum_min_TIPO1',\n",
      "       'mean_min_TIPO1', 'sum_gigas_TIPO2', 'mean_gigas_TIPO2',\n",
      "       'sum_sms_TIPO1', 'mean_sms_TIPO1'],\n",
      "      dtype='object')\n",
      "(547995, 394)\n",
      "(186549, 394)\n",
      "(547995, 382)\n",
      "(186549, 382)\n",
      "(547995, 382)\n",
      "(186549, 382)\n"
     ]
    }
   ],
   "source": [
    "# eliminamos valores unicos\n",
    "\n",
    "cols_with_onlyone_val = train.columns[train.nunique() == 1]\n",
    "print(cols_with_onlyone_val)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\n",
    "test.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "NUM_OF_DECIMALS = 32\n",
    "train = train.round(NUM_OF_DECIMALS)\n",
    "test = test.round(NUM_OF_DECIMALS)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547995, 371)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsToRemove = []\n",
    "columns = train.columns\n",
    "for i in range(len(columns)-1):\n",
    "    v = train[columns[i]].values\n",
    "    dupCols = []\n",
    "    for j in range(i + 1,len(columns)):\n",
    "        if np.array_equal(v, train[columns[j]].values):\n",
    "            colsToRemove.append(columns[j])\n",
    "train.drop(colsToRemove, axis=1, inplace=True) \n",
    "test.drop(colsToRemove, axis=1, inplace=True) \n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la base tiene 371 columns.\n",
      "estos son 302 columnas que tienen valores perdidos.\n",
      "la base tiene 371 columns.\n",
      "estos son 302 columnas que tienen valores perdidos.\n"
     ]
    }
   ],
   "source": [
    "missing_train = missing_values_table(train)\n",
    "missing_train_vars = list(missing_train.index[missing_train['% de valores totales'] > 90])\n",
    "missing_test = missing_values_table(test)\n",
    "missing_test_vars = list(missing_test.index[missing_test['% de valores totales'] > 90])\n",
    "missing_columns = list(set(missing_test_vars + missing_train_vars))\n",
    "# Drop the missing columns\n",
    "train = train.drop(columns = missing_columns)\n",
    "test = test.drop(columns = missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import random_projection\n",
    "\n",
    "weight = ((train != 0).sum()/len(train)).values\n",
    "tmp_train = train[train!=0]\n",
    "tmp_test = test[test!=0]\n",
    "tmp = pd.concat([train,test])#RandomProjection\n",
    "ntrain = len(train)\n",
    "ntest = len(test)\n",
    "\n",
    "train[\"weight_count\"] = (tmp_train*weight).sum(axis=1)\n",
    "test[\"weight_count\"] = (tmp_test*weight).sum(axis=1)\n",
    "\n",
    "train[\"count_not0\"] = (train != 0).sum(axis=1)\n",
    "test[\"count_not0\"] = (test != 0).sum(axis=1)\n",
    "\n",
    "train[\"sum\"] = train.sum(axis=1)\n",
    "test[\"sum\"] = test.sum(axis=1)\n",
    "\n",
    "train[\"var\"] = tmp_train.var(axis=1)\n",
    "test[\"var\"] = tmp_test.var(axis=1)\n",
    "\n",
    "train[\"mean\"] = tmp_train.mean(axis=1)\n",
    "test[\"mean\"] = tmp_test.mean(axis=1)\n",
    "\n",
    "train[\"std\"] = tmp_train.std(axis=1)\n",
    "test[\"std\"] = tmp_test.std(axis=1)\n",
    "\n",
    "train[\"max\"] = tmp_train.max(axis=1)\n",
    "test[\"max\"] = tmp_test.max(axis=1)\n",
    "\n",
    "train[\"min\"] = tmp_train.min(axis=1)\n",
    "test[\"min\"] = tmp_test.min(axis=1)\n",
    "#######################\n",
    "#train[\"median\"] = tmp_train.median(axis=1)\n",
    "#test[\"median\"] = tmp_test.median(axis=1)\n",
    "\n",
    "#train[\"skew\"] = tmp_train.skew(axis=1)\n",
    "#test[\"skew\"] = tmp_test.skew(axis=1)\n",
    "\n",
    "#train[\"kurtosis\"] = tmp_train.kurtosis(axis=1)\n",
    "#test[\"kurtosis\"] = tmp_test.kurtosis(axis=1)\n",
    "\n",
    "del(tmp_train)\n",
    "del(tmp_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.stats import ks_2samp\n",
    "THRESHOLD_P_VALUE = 0.01 #need tuned\n",
    "THRESHOLD_STATISTIC = 0.3 #need tuned\n",
    "diff_cols = []\n",
    "for col in train.columns:\n",
    "    statistic, pvalue = ks_2samp(train[col].values, test[col].values)\n",
    "    if pvalue <= THRESHOLD_P_VALUE and np.abs(statistic) > THRESHOLD_STATISTIC:\n",
    "        diff_cols.append(col)\n",
    "for col in diff_cols:\n",
    "    if col in train.columns:\n",
    "        train.drop(col, axis=1, inplace=True)\n",
    "        test.drop(col, axis=1, inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test['full_count_null'] = test.apply(lambda x: x.count(), axis=1)\n",
    "train['full_count_null'] = train.apply(lambda x: x.count(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (547995, 356) Testing (186549, 356)\n"
     ]
    }
   ],
   "source": [
    "print('Training',train.shape,'Testing',test.shape) ###(358487, 2630) (396666, 2630)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_child_samples  : Número mínimo de datos necesarios en un niño (hoja). Según los documentos de LightGBM, este es un parámetro muy importante para evitar el sobreajuste.\n",
    "\n",
    "#### num_leaves (LightGBM): máximo de hojas de árboles para los estudiantes básicos. Un valor más alto da como resultado árboles más profundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelo de lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.822498\ttraining's binary_logloss: 0.0879256\tvalid_1's auc: 0.617626\tvalid_1's binary_logloss: 0.143823\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's auc: 0.830051\ttraining's binary_logloss: 0.087074\tvalid_1's auc: 0.621344\tvalid_1's binary_logloss: 0.143556\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.806013\ttraining's binary_logloss: 0.0978154\tvalid_1's auc: 0.736823\tvalid_1's binary_logloss: 0.0994433\n",
      "[100]\ttraining's auc: 0.823727\ttraining's binary_logloss: 0.0955714\tvalid_1's auc: 0.744761\tvalid_1's binary_logloss: 0.0985741\n",
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's auc: 0.834453\ttraining's binary_logloss: 0.0942406\tvalid_1's auc: 0.748097\tvalid_1's binary_logloss: 0.098281\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.788798\ttraining's binary_logloss: 0.100916\tvalid_1's auc: 0.825893\tvalid_1's binary_logloss: 0.0872616\n",
      "[100]\ttraining's auc: 0.807182\ttraining's binary_logloss: 0.0987865\tvalid_1's auc: 0.82979\tvalid_1's binary_logloss: 0.0862379\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's auc: 0.814609\ttraining's binary_logloss: 0.0979336\tvalid_1's auc: 0.831406\tvalid_1's binary_logloss: 0.0860896\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.788707\ttraining's binary_logloss: 0.103475\tvalid_1's auc: 0.827412\tvalid_1's binary_logloss: 0.0775155\n",
      "[100]\ttraining's auc: 0.805978\ttraining's binary_logloss: 0.101217\tvalid_1's auc: 0.833322\tvalid_1's binary_logloss: 0.0763596\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's auc: 0.8153\ttraining's binary_logloss: 0.100156\tvalid_1's auc: 0.834122\tvalid_1's binary_logloss: 0.0760661\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.798091\ttraining's binary_logloss: 0.0984707\tvalid_1's auc: 0.789025\tvalid_1's binary_logloss: 0.0993494\n",
      "[100]\ttraining's auc: 0.816507\ttraining's binary_logloss: 0.096101\tvalid_1's auc: 0.792986\tvalid_1's binary_logloss: 0.0984756\n",
      "Early stopping, best iteration is:\n",
      "[93]\ttraining's auc: 0.814538\ttraining's binary_logloss: 0.0963634\tvalid_1's auc: 0.79351\tvalid_1's binary_logloss: 0.0984921\n",
      "*********************\n",
      "roc auc estimado:  0.7763285462753148\n"
     ]
    }
   ],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs = []\n",
    "train_probs = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "train_probs = pd.concat(train_probs)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's auc: 0.826138\ttraining's binary_logloss: 0.0880913\tvalid_1's auc: 0.613949\tvalid_1's binary_logloss: 0.143739\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.828299\ttraining's binary_logloss: 0.0953125\tvalid_1's auc: 0.743605\tvalid_1's binary_logloss: 0.0987703\n",
      "[100]\ttraining's auc: 0.853781\ttraining's binary_logloss: 0.0918041\tvalid_1's auc: 0.747437\tvalid_1's binary_logloss: 0.0982879\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's auc: 0.850204\ttraining's binary_logloss: 0.0922917\tvalid_1's auc: 0.748907\tvalid_1's binary_logloss: 0.0982367\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.811329\ttraining's binary_logloss: 0.0986056\tvalid_1's auc: 0.827459\tvalid_1's binary_logloss: 0.0865063\n",
      "[100]\ttraining's auc: 0.839467\ttraining's binary_logloss: 0.0949693\tvalid_1's auc: 0.832994\tvalid_1's binary_logloss: 0.0856865\n",
      "Early stopping, best iteration is:\n",
      "[113]\ttraining's auc: 0.84447\ttraining's binary_logloss: 0.0942808\tvalid_1's auc: 0.833367\tvalid_1's binary_logloss: 0.0855952\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.809644\ttraining's binary_logloss: 0.100954\tvalid_1's auc: 0.832995\tvalid_1's binary_logloss: 0.076138\n",
      "[100]\ttraining's auc: 0.83639\ttraining's binary_logloss: 0.0975412\tvalid_1's auc: 0.835684\tvalid_1's binary_logloss: 0.0753626\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's auc: 0.841311\ttraining's binary_logloss: 0.0969225\tvalid_1's auc: 0.836192\tvalid_1's binary_logloss: 0.0752736\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.819964\ttraining's binary_logloss: 0.0957795\tvalid_1's auc: 0.790717\tvalid_1's binary_logloss: 0.0982355\n",
      "Early stopping, best iteration is:\n",
      "[87]\ttraining's auc: 0.839994\ttraining's binary_logloss: 0.0931872\tvalid_1's auc: 0.793976\tvalid_1's binary_logloss: 0.0976006\n",
      "*********************\n",
      "roc auc estimado:  0.7775878125394572\n"
     ]
    }
   ],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs1 = []\n",
    "train_probs1 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= 8 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=60 #100\n",
    "                            ,num_leaves=20  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs1.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs1.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs1 = pd.concat(test_probs1, axis=1).mean(axis=1)\n",
    "train_probs1 = pd.concat(train_probs1)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs1.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.847336\ttraining's binary_logloss: 0.084632\tvalid_1's auc: 0.624017\tvalid_1's binary_logloss: 0.143395\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's auc: 0.840439\ttraining's binary_logloss: 0.085781\tvalid_1's auc: 0.623392\tvalid_1's binary_logloss: 0.143271\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.831453\ttraining's binary_logloss: 0.0945707\tvalid_1's auc: 0.743674\tvalid_1's binary_logloss: 0.0986884\n",
      "[100]\ttraining's auc: 0.858388\ttraining's binary_logloss: 0.090586\tvalid_1's auc: 0.747482\tvalid_1's binary_logloss: 0.0982787\n",
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's auc: 0.853715\ttraining's binary_logloss: 0.0912938\tvalid_1's auc: 0.747745\tvalid_1's binary_logloss: 0.0982989\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.813227\ttraining's binary_logloss: 0.0978778\tvalid_1's auc: 0.827798\tvalid_1's binary_logloss: 0.0863165\n",
      "[100]\ttraining's auc: 0.842266\ttraining's binary_logloss: 0.0939575\tvalid_1's auc: 0.831293\tvalid_1's binary_logloss: 0.0856842\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's auc: 0.838741\ttraining's binary_logloss: 0.0944998\tvalid_1's auc: 0.831928\tvalid_1's binary_logloss: 0.0856877\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.814031\ttraining's binary_logloss: 0.10027\tvalid_1's auc: 0.831401\tvalid_1's binary_logloss: 0.0768881\n",
      "[100]\ttraining's auc: 0.843089\ttraining's binary_logloss: 0.0964086\tvalid_1's auc: 0.836077\tvalid_1's binary_logloss: 0.0760836\n",
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's auc: 0.837871\ttraining's binary_logloss: 0.0970871\tvalid_1's auc: 0.836141\tvalid_1's binary_logloss: 0.0760535\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.824117\ttraining's binary_logloss: 0.0951085\tvalid_1's auc: 0.791622\tvalid_1's binary_logloss: 0.0978193\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttraining's auc: 0.845733\ttraining's binary_logloss: 0.0921257\tvalid_1's auc: 0.795029\tvalid_1's binary_logloss: 0.0970326\n",
      "*********************\n",
      "roc auc estimado:  0.778027652165151\n"
     ]
    }
   ],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs2 = []\n",
    "train_probs2 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=50 #100\n",
    "                            ,num_leaves=20  #20\n",
    "                            #,reg_alpha=0 # 0\n",
    "                            #,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs2.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs2.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs2 = pd.concat(test_probs2, axis=1).mean(axis=1)\n",
    "train_probs2 = pd.concat(train_probs2)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs2.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.8073\ttraining's binary_logloss: 0.0904063\tvalid_1's auc: 0.608766\tvalid_1's binary_logloss: 0.144347\n",
      "[100]\ttraining's auc: 0.823789\ttraining's binary_logloss: 0.0880333\tvalid_1's auc: 0.619816\tvalid_1's binary_logloss: 0.143509\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's auc: 0.823353\ttraining's binary_logloss: 0.0881084\tvalid_1's auc: 0.620327\tvalid_1's binary_logloss: 0.143463\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.78954\ttraining's binary_logloss: 0.100336\tvalid_1's auc: 0.725234\tvalid_1's binary_logloss: 0.101213\n",
      "[100]\ttraining's auc: 0.806358\ttraining's binary_logloss: 0.0979432\tvalid_1's auc: 0.736497\tvalid_1's binary_logloss: 0.0996577\n",
      "[150]\ttraining's auc: 0.816921\ttraining's binary_logloss: 0.096626\tvalid_1's auc: 0.741788\tvalid_1's binary_logloss: 0.0989995\n",
      "[200]\ttraining's auc: 0.825078\ttraining's binary_logloss: 0.0956424\tvalid_1's auc: 0.744852\tvalid_1's binary_logloss: 0.0986387\n",
      "[250]\ttraining's auc: 0.832268\ttraining's binary_logloss: 0.0948065\tvalid_1's auc: 0.746216\tvalid_1's binary_logloss: 0.0985085\n",
      "[300]\ttraining's auc: 0.838228\ttraining's binary_logloss: 0.0940129\tvalid_1's auc: 0.747134\tvalid_1's binary_logloss: 0.098412\n",
      "[350]\ttraining's auc: 0.84377\ttraining's binary_logloss: 0.0932667\tvalid_1's auc: 0.748829\tvalid_1's binary_logloss: 0.098272\n",
      "Early stopping, best iteration is:\n",
      "[369]\ttraining's auc: 0.845538\ttraining's binary_logloss: 0.0930011\tvalid_1's auc: 0.749225\tvalid_1's binary_logloss: 0.0982253\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.770983\ttraining's binary_logloss: 0.103321\tvalid_1's auc: 0.815651\tvalid_1's binary_logloss: 0.0891492\n",
      "[100]\ttraining's auc: 0.78796\ttraining's binary_logloss: 0.101123\tvalid_1's auc: 0.824692\tvalid_1's binary_logloss: 0.0874343\n",
      "[150]\ttraining's auc: 0.798999\ttraining's binary_logloss: 0.0998129\tvalid_1's auc: 0.828007\tvalid_1's binary_logloss: 0.0867372\n",
      "[200]\ttraining's auc: 0.80755\ttraining's binary_logloss: 0.0988088\tvalid_1's auc: 0.830259\tvalid_1's binary_logloss: 0.0863513\n",
      "Early stopping, best iteration is:\n",
      "[203]\ttraining's auc: 0.808083\ttraining's binary_logloss: 0.0987536\tvalid_1's auc: 0.830299\tvalid_1's binary_logloss: 0.0863359\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.770978\ttraining's binary_logloss: 0.105833\tvalid_1's auc: 0.819602\tvalid_1's binary_logloss: 0.0791593\n",
      "[100]\ttraining's auc: 0.789052\ttraining's binary_logloss: 0.103511\tvalid_1's auc: 0.828817\tvalid_1's binary_logloss: 0.0771036\n",
      "[150]\ttraining's auc: 0.798772\ttraining's binary_logloss: 0.10226\tvalid_1's auc: 0.831496\tvalid_1's binary_logloss: 0.0764342\n",
      "[200]\ttraining's auc: 0.806728\ttraining's binary_logloss: 0.101297\tvalid_1's auc: 0.833237\tvalid_1's binary_logloss: 0.076064\n",
      "[250]\ttraining's auc: 0.813874\ttraining's binary_logloss: 0.100439\tvalid_1's auc: 0.835211\tvalid_1's binary_logloss: 0.075767\n",
      "Early stopping, best iteration is:\n",
      "[246]\ttraining's auc: 0.813362\ttraining's binary_logloss: 0.100502\tvalid_1's auc: 0.835335\tvalid_1's binary_logloss: 0.0757925\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.780004\ttraining's binary_logloss: 0.100812\tvalid_1's auc: 0.784231\tvalid_1's binary_logloss: 0.102131\n",
      "[100]\ttraining's auc: 0.798532\ttraining's binary_logloss: 0.0984758\tvalid_1's auc: 0.790852\tvalid_1's binary_logloss: 0.0996573\n",
      "[150]\ttraining's auc: 0.810895\ttraining's binary_logloss: 0.0971178\tvalid_1's auc: 0.793536\tvalid_1's binary_logloss: 0.0988497\n",
      "[200]\ttraining's auc: 0.818578\ttraining's binary_logloss: 0.0961119\tvalid_1's auc: 0.795006\tvalid_1's binary_logloss: 0.0985137\n",
      "[250]\ttraining's auc: 0.825718\ttraining's binary_logloss: 0.095189\tvalid_1's auc: 0.796006\tvalid_1's binary_logloss: 0.0982091\n",
      "Early stopping, best iteration is:\n",
      "[257]\ttraining's auc: 0.826508\ttraining's binary_logloss: 0.0950611\tvalid_1's auc: 0.79637\tvalid_1's binary_logloss: 0.0980548\n",
      "*********************\n",
      "roc auc estimado:  0.777544742880063\n"
     ]
    }
   ],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs3 = []\n",
    "train_probs3 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            ,learning_rate=0.05\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs3.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs3.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs3 = pd.concat(test_probs3, axis=1).mean(axis=1)\n",
    "train_probs3 = pd.concat(train_probs3)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs3.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's auc: 0.810073\ttraining's binary_logloss: 0.510892\tvalid_1's auc: 0.611495\tvalid_1's binary_logloss: 0.610033\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.807715\ttraining's binary_logloss: 0.510954\tvalid_1's auc: 0.740378\tvalid_1's binary_logloss: 0.502806\n",
      "[100]\ttraining's auc: 0.825112\ttraining's binary_logloss: 0.49407\tvalid_1's auc: 0.748995\tvalid_1's binary_logloss: 0.480597\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttraining's auc: 0.833993\ttraining's binary_logloss: 0.486386\tvalid_1's auc: 0.750955\tvalid_1's binary_logloss: 0.475228\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.788387\ttraining's binary_logloss: 0.537332\tvalid_1's auc: 0.825867\tvalid_1's binary_logloss: 0.428753\n",
      "[100]\ttraining's auc: 0.806109\ttraining's binary_logloss: 0.520109\tvalid_1's auc: 0.830559\tvalid_1's binary_logloss: 0.399486\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's auc: 0.80353\ttraining's binary_logloss: 0.522294\tvalid_1's auc: 0.830864\tvalid_1's binary_logloss: 0.402696\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.788057\ttraining's binary_logloss: 0.536151\tvalid_1's auc: 0.827769\tvalid_1's binary_logloss: 0.412142\n",
      "[100]\ttraining's auc: 0.806297\ttraining's binary_logloss: 0.519128\tvalid_1's auc: 0.833003\tvalid_1's binary_logloss: 0.390591\n",
      "Early stopping, best iteration is:\n",
      "[108]\ttraining's auc: 0.808584\ttraining's binary_logloss: 0.516953\tvalid_1's auc: 0.833765\tvalid_1's binary_logloss: 0.386802\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.798483\ttraining's binary_logloss: 0.528126\tvalid_1's auc: 0.791815\tvalid_1's binary_logloss: 0.463161\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's auc: 0.804522\ttraining's binary_logloss: 0.522126\tvalid_1's auc: 0.793726\tvalid_1's binary_logloss: 0.458036\n",
      "*********************\n",
      "roc auc estimado:  0.7761234907777911\n"
     ]
    }
   ],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs4 = []\n",
    "train_probs4 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            ,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs4.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs4.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs4 = pd.concat(test_probs4, axis=1).mean(axis=1)\n",
    "train_probs4 = pd.concat(train_probs4)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs4.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.806058\ttraining's binary_logloss: 0.0905278\tvalid_1's auc: 0.608371\tvalid_1's binary_logloss: 0.14441\n",
      "[100]\ttraining's auc: 0.821751\ttraining's binary_logloss: 0.088327\tvalid_1's auc: 0.618658\tvalid_1's binary_logloss: 0.143655\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's auc: 0.8295\ttraining's binary_logloss: 0.0873629\tvalid_1's auc: 0.622772\tvalid_1's binary_logloss: 0.143416\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.788864\ttraining's binary_logloss: 0.10042\tvalid_1's auc: 0.724904\tvalid_1's binary_logloss: 0.101315\n",
      "[100]\ttraining's auc: 0.8052\ttraining's binary_logloss: 0.0980945\tvalid_1's auc: 0.736471\tvalid_1's binary_logloss: 0.099688\n",
      "[150]\ttraining's auc: 0.814594\ttraining's binary_logloss: 0.0968206\tvalid_1's auc: 0.74171\tvalid_1's binary_logloss: 0.0989898\n",
      "[200]\ttraining's auc: 0.821867\ttraining's binary_logloss: 0.0959367\tvalid_1's auc: 0.745796\tvalid_1's binary_logloss: 0.0985958\n",
      "[250]\ttraining's auc: 0.827948\ttraining's binary_logloss: 0.0952\tvalid_1's auc: 0.747427\tvalid_1's binary_logloss: 0.098452\n",
      "[300]\ttraining's auc: 0.833692\ttraining's binary_logloss: 0.094506\tvalid_1's auc: 0.748816\tvalid_1's binary_logloss: 0.0983209\n",
      "Early stopping, best iteration is:\n",
      "[315]\ttraining's auc: 0.835217\ttraining's binary_logloss: 0.0942954\tvalid_1's auc: 0.749244\tvalid_1's binary_logloss: 0.0982671\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.769317\ttraining's binary_logloss: 0.103381\tvalid_1's auc: 0.814889\tvalid_1's binary_logloss: 0.089299\n",
      "[100]\ttraining's auc: 0.78664\ttraining's binary_logloss: 0.101243\tvalid_1's auc: 0.825563\tvalid_1's binary_logloss: 0.0874607\n",
      "[150]\ttraining's auc: 0.795938\ttraining's binary_logloss: 0.10009\tvalid_1's auc: 0.829265\tvalid_1's binary_logloss: 0.0866554\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's auc: 0.800078\ttraining's binary_logloss: 0.099607\tvalid_1's auc: 0.83027\tvalid_1's binary_logloss: 0.0864312\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.769096\ttraining's binary_logloss: 0.10589\tvalid_1's auc: 0.818268\tvalid_1's binary_logloss: 0.0791276\n",
      "[100]\ttraining's auc: 0.786909\ttraining's binary_logloss: 0.103736\tvalid_1's auc: 0.8291\tvalid_1's binary_logloss: 0.0771928\n",
      "[150]\ttraining's auc: 0.79658\ttraining's binary_logloss: 0.10253\tvalid_1's auc: 0.831467\tvalid_1's binary_logloss: 0.0765511\n",
      "[200]\ttraining's auc: 0.804342\ttraining's binary_logloss: 0.101622\tvalid_1's auc: 0.833106\tvalid_1's binary_logloss: 0.0761887\n",
      "Early stopping, best iteration is:\n",
      "[231]\ttraining's auc: 0.808803\ttraining's binary_logloss: 0.101143\tvalid_1's auc: 0.834737\tvalid_1's binary_logloss: 0.0760317\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.778926\ttraining's binary_logloss: 0.100908\tvalid_1's auc: 0.782942\tvalid_1's binary_logloss: 0.101882\n",
      "[100]\ttraining's auc: 0.796545\ttraining's binary_logloss: 0.0986622\tvalid_1's auc: 0.790495\tvalid_1's binary_logloss: 0.0994669\n",
      "[150]\ttraining's auc: 0.80763\ttraining's binary_logloss: 0.0973979\tvalid_1's auc: 0.793557\tvalid_1's binary_logloss: 0.0987482\n",
      "Early stopping, best iteration is:\n",
      "[151]\ttraining's auc: 0.80771\ttraining's binary_logloss: 0.0973825\tvalid_1's auc: 0.793588\tvalid_1's binary_logloss: 0.0987376\n",
      "*********************\n",
      "roc auc estimado:  0.7764349610361232\n"
     ]
    }
   ],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs5 = []\n",
    "train_probs5 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= 5 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            ,learning_rate=0.05\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs5.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs5.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs5 = pd.concat(test_probs5, axis=1).mean(axis=1)\n",
    "train_probs5 = pd.concat(train_probs5)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs5.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.822913\ttraining's binary_logloss: 0.0879158\tvalid_1's auc: 0.623681\tvalid_1's binary_logloss: 0.143018\n",
      "[100]\ttraining's auc: 0.843008\ttraining's binary_logloss: 0.0854279\tvalid_1's auc: 0.629811\tvalid_1's binary_logloss: 0.142696\n",
      "[150]\ttraining's auc: 0.854242\ttraining's binary_logloss: 0.0837696\tvalid_1's auc: 0.631739\tvalid_1's binary_logloss: 0.142609\n",
      "Early stopping, best iteration is:\n",
      "[152]\ttraining's auc: 0.854779\ttraining's binary_logloss: 0.0837165\tvalid_1's auc: 0.631929\tvalid_1's binary_logloss: 0.142601\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.806092\ttraining's binary_logloss: 0.0977597\tvalid_1's auc: 0.737165\tvalid_1's binary_logloss: 0.0994104\n",
      "[100]\ttraining's auc: 0.82562\ttraining's binary_logloss: 0.0953692\tvalid_1's auc: 0.744175\tvalid_1's binary_logloss: 0.0985724\n",
      "[150]\ttraining's auc: 0.837727\ttraining's binary_logloss: 0.0937448\tvalid_1's auc: 0.747884\tvalid_1's binary_logloss: 0.0982504\n",
      "[200]\ttraining's auc: 0.847282\ttraining's binary_logloss: 0.0923175\tvalid_1's auc: 0.749508\tvalid_1's binary_logloss: 0.0980586\n",
      "[250]\ttraining's auc: 0.855536\ttraining's binary_logloss: 0.0910714\tvalid_1's auc: 0.751954\tvalid_1's binary_logloss: 0.0978781\n",
      "[300]\ttraining's auc: 0.863068\ttraining's binary_logloss: 0.0899056\tvalid_1's auc: 0.753055\tvalid_1's binary_logloss: 0.0978108\n",
      "Early stopping, best iteration is:\n",
      "[294]\ttraining's auc: 0.862116\ttraining's binary_logloss: 0.090056\tvalid_1's auc: 0.753018\tvalid_1's binary_logloss: 0.0977973\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.787137\ttraining's binary_logloss: 0.100987\tvalid_1's auc: 0.826157\tvalid_1's binary_logloss: 0.0871013\n",
      "[100]\ttraining's auc: 0.807082\ttraining's binary_logloss: 0.0986814\tvalid_1's auc: 0.83041\tvalid_1's binary_logloss: 0.0860568\n",
      "[150]\ttraining's auc: 0.821545\ttraining's binary_logloss: 0.096974\tvalid_1's auc: 0.832848\tvalid_1's binary_logloss: 0.0857123\n",
      "[200]\ttraining's auc: 0.832626\ttraining's binary_logloss: 0.0955242\tvalid_1's auc: 0.83588\tvalid_1's binary_logloss: 0.0854632\n",
      "[250]\ttraining's auc: 0.841875\ttraining's binary_logloss: 0.0942289\tvalid_1's auc: 0.836451\tvalid_1's binary_logloss: 0.0853691\n",
      "Early stopping, best iteration is:\n",
      "[232]\ttraining's auc: 0.838618\ttraining's binary_logloss: 0.0946918\tvalid_1's auc: 0.83675\tvalid_1's binary_logloss: 0.085354\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.789679\ttraining's binary_logloss: 0.103449\tvalid_1's auc: 0.827467\tvalid_1's binary_logloss: 0.0775012\n",
      "[100]\ttraining's auc: 0.80691\ttraining's binary_logloss: 0.101172\tvalid_1's auc: 0.831231\tvalid_1's binary_logloss: 0.0763951\n",
      "[150]\ttraining's auc: 0.820538\ttraining's binary_logloss: 0.0994918\tvalid_1's auc: 0.832572\tvalid_1's binary_logloss: 0.0759576\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's auc: 0.815829\ttraining's binary_logloss: 0.100097\tvalid_1's auc: 0.83322\tvalid_1's binary_logloss: 0.0760279\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[50]\ttraining's auc: 0.79915\ttraining's binary_logloss: 0.0984012\tvalid_1's auc: 0.789471\tvalid_1's binary_logloss: 0.0994775\n",
      "[100]\ttraining's auc: 0.817379\ttraining's binary_logloss: 0.0960689\tvalid_1's auc: 0.795773\tvalid_1's binary_logloss: 0.0980583\n",
      "[150]\ttraining's auc: 0.830742\ttraining's binary_logloss: 0.0943987\tvalid_1's auc: 0.796704\tvalid_1's binary_logloss: 0.0977538\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttraining's auc: 0.826219\ttraining's binary_logloss: 0.0949568\tvalid_1's auc: 0.797022\tvalid_1's binary_logloss: 0.0977229\n",
      "*********************\n",
      "roc auc estimado:  0.7810625893790332\n"
     ]
    }
   ],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs6 = []\n",
    "train_probs6 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=50 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=20, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs6.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs6.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs6 = pd.concat(test_probs6, axis=1).mean(axis=1)\n",
    "train_probs6 = pd.concat(train_probs6)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs6.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_envio=test_probs6.copy()\n",
    "test_envio.name = \"TARGET\"\n",
    "test_envio=pd.DataFrame(test_envio).reset_index()\n",
    "#test_envio['nro_telefono_hash']=0\n",
    "test_envio['nro_telefono_hash']=test_envio.key_cliente.str.slice(7,500)\n",
    "#test_envio=pd.merge(test_envio,df_full_hist[['key_cliente','nro_telefono_hash']],on=['key_cliente'],how='left')\n",
    "test_envio.drop('key_cliente', axis=1,inplace=True)\n",
    "summission=test_envio[['nro_telefono_hash','TARGET']].set_index('nro_telefono_hash')\n",
    "summission.to_csv('summission_T6.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's auc: 0.807281\ttraining's binary_logloss: 0.0902684\tvalid_1's auc: 0.609332\tvalid_1's binary_logloss: 0.144376\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.811695\ttraining's binary_logloss: 0.0972836\tvalid_1's auc: 0.737995\tvalid_1's binary_logloss: 0.0997489\n",
      "[100]\ttraining's auc: 0.830714\ttraining's binary_logloss: 0.0946661\tvalid_1's auc: 0.745586\tvalid_1's binary_logloss: 0.0988559\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's auc: 0.833909\ttraining's binary_logloss: 0.0942834\tvalid_1's auc: 0.746543\tvalid_1's binary_logloss: 0.0987599\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.793032\ttraining's binary_logloss: 0.100508\tvalid_1's auc: 0.825651\tvalid_1's binary_logloss: 0.0874058\n",
      "[100]\ttraining's auc: 0.812217\ttraining's binary_logloss: 0.0980204\tvalid_1's auc: 0.83186\tvalid_1's binary_logloss: 0.0860919\n",
      "[150]\ttraining's auc: 0.82785\ttraining's binary_logloss: 0.0960473\tvalid_1's auc: 0.834971\tvalid_1's binary_logloss: 0.0855423\n",
      "Early stopping, best iteration is:\n",
      "[182]\ttraining's auc: 0.835914\ttraining's binary_logloss: 0.0949648\tvalid_1's auc: 0.835614\tvalid_1's binary_logloss: 0.0853619\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.793377\ttraining's binary_logloss: 0.102982\tvalid_1's auc: 0.825994\tvalid_1's binary_logloss: 0.0773188\n",
      "[100]\ttraining's auc: 0.814224\ttraining's binary_logloss: 0.100215\tvalid_1's auc: 0.831573\tvalid_1's binary_logloss: 0.0763275\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's auc: 0.82014\ttraining's binary_logloss: 0.0994241\tvalid_1's auc: 0.832736\tvalid_1's binary_logloss: 0.076184\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.804365\ttraining's binary_logloss: 0.0977725\tvalid_1's auc: 0.788382\tvalid_1's binary_logloss: 0.0999418\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's auc: 0.812074\ttraining's binary_logloss: 0.0967139\tvalid_1's auc: 0.790571\tvalid_1's binary_logloss: 0.099559\n",
      "*********************\n",
      "roc auc estimado:  0.775249673725476\n"
     ]
    }
   ],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs7 = []\n",
    "train_probs7 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            #,max_bin = 8\n",
    "                            #,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs7.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs7.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs7 = pd.concat(test_probs7, axis=1).mean(axis=1)\n",
    "train_probs7 = pd.concat(train_probs7)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs7.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emsamble promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_probs1=(test_probs + test_probs1 + test_probs2 + test_probs3 +test_probs4+test_probs5+ test_probs6 + test_probs7 )/8\n",
    "#print(test_probs_apv.shape,test_probs.shape)\n",
    "\n",
    "test_envio=ensembled_probs1.copy()\n",
    "test_envio.name = \"TARGET\"\n",
    "test_envio=pd.DataFrame(test_envio).reset_index()\n",
    "#test_envio['nro_telefono_hash']=0\n",
    "test_envio['nro_telefono_hash']=test_envio.key_cliente.str.slice(7,500)\n",
    "#test_envio=pd.merge(test_envio,df_full_hist[['key_cliente','nro_telefono_hash']],on=['key_cliente'],how='left')\n",
    "test_envio.drop('key_cliente', axis=1,inplace=True)\n",
    "summission=test_envio[['nro_telefono_hash','TARGET']].set_index('nro_telefono_hash')\n",
    "summission.to_csv('summission_lgbms_prom.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emsamble de modelos con otro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train\n",
    "X_test=test\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidatos =[{\n",
    "        \"learner\": LGBMClassifier,\n",
    "        \"param_grid\": model_selection.ParameterGrid({\n",
    "               \"n_estimators\": [10000],\n",
    "               \"num_leaves\": [10,20], # [ 10,20,40,60, 100],\n",
    "               \"max_depth\": [-1,3,5,10], #[-1, 5, 8, 10],\n",
    "               \"min_child_samples\": [100,50,20 ],#[10,20,30,60,100],\n",
    "        }),\n",
    "        \"train_params\": {\n",
    "            \"early_stopping_rounds\": 10,\n",
    "            \"eval_metric\": \"auc\", \n",
    "            \"eval_set\": True,\n",
    "            \"verbose\": 50\n",
    "        } \n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(model_selection.KFold(n_splits=5, shuffle=True, random_state=123).split(X_train))\n",
    "res = []\n",
    "bestRes = 0\n",
    "bestProbs = y_train\n",
    "for candidate in candidatos:\n",
    "    for params in candidate[\"param_grid\"]:\n",
    "        trainParams = candidate.get(\"train_params\", {})\n",
    "        valid_probs = []\n",
    "        test_probs = []\n",
    "        for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "            Xt = X_train.iloc[train_idx]\n",
    "            yt = y_train.loc[X_train.index].iloc[train_idx]\n",
    "\n",
    "            Xv = X_train.iloc[valid_idx]\n",
    "            yv = y_train.loc[X_train.index].iloc[valid_idx]\n",
    "            if \"eval_set\" in trainParams:\n",
    "                trainParams[\"eval_set\"] = [(Xt, yt), (Xv, yv)]\n",
    "            learner = candidate[\"learner\"](**params)\n",
    "            learner.fit(Xt, yt, **trainParams)\n",
    "\n",
    "            valid_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"SCORE\"))\n",
    "            test_probs.append(pd.Series(learner.predict_proba(X_test)[:, -1],\n",
    "                                        index=X_test.index, name=\"fold_\" + str(i)))\n",
    "\n",
    "        valid_probs = pd.concat(valid_probs)\n",
    "        cres = roc_auc_score(y_train, valid_probs.loc[y_train.index])\n",
    "        cols = [\"learner\"]\n",
    "        vals = [candidate[\"learner\"].__name__]\n",
    "        for p in params:\n",
    "            cols.append(\"param_\" + p)\n",
    "            vals.append(params[p])\n",
    "        cols.append(\"score\")\n",
    "        vals.append(cres)\n",
    "        res.append(pd.DataFrame([vals], columns=cols))\n",
    "        print(\"*\"*10)\n",
    "        print(pd.concat(res))\n",
    "        print(\"*\"*10)\n",
    "        if cres > bestRes:\n",
    "            test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "            test_probs.index.name=\"key_value\"\n",
    "            test_probs.name=\"target\"\n",
    "            bestProbs = test_probs\n",
    "            bestRes = cres\n",
    "\n",
    "#bestProbs.to_csv(\"benchmark.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_envio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_Score=pd.concat(res).sort_values('score')#[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_Score[160:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_envio=bestProbs.copy()\n",
    "test_envio.name = \"TARGET\"\n",
    "test_envio=pd.DataFrame(test_envio).reset_index()\n",
    "#test_envio['nro_telefono_hash']=0\n",
    "test_envio['nro_telefono_hash']=test_envio.key_value.str.slice(7,500)\n",
    "#test_envio=pd.merge(test_envio,df_full_hist[['key_cliente','nro_telefono_hash']],on=['key_cliente'],how='left')\n",
    "test_envio.drop('key_value', axis=1,inplace=True)\n",
    "summission=test_envio[['nro_telefono_hash','TARGET']].set_index('nro_telefono_hash')\n",
    "summission.to_csv('summission_Tuning.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summission.to_csv('summission_Tuning.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
