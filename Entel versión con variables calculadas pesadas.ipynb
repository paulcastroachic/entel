{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',0)\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# for variable transformation\n",
    "import scipy.stats as stats\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm_notebook\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from scipy.optimize import differential_evolution\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "from scipy.optimize import differential_evolution\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score\n",
    "from scikitplot.estimators import plot_feature_importances\n",
    "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics as metrics\n",
    "from datetime import datetime\n",
    "import re\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_month(df,cols,newcol=False):\n",
    "    for col in cols:\n",
    "        if newcol:\n",
    "            cn = col+'_parsed' \n",
    "        else: cn = col\n",
    "        df[cn] = pd.to_datetime(df[col],format='%Y%m')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "          # Total valores perdidos\n",
    "        mis_val = df.isnull().sum()\n",
    "         # porcentaje de valores perdidos\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "         # crea una tabla con los resultados\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        # renombra columnas\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'valores perdidos', 1 : '% de valores totales'})\n",
    "       # ordena de mayor a menor los porcentaje de valores perdidos\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% de valores totales', ascending=False).round(1)\n",
    "        # te muestra la informacion resumida\n",
    "        print (\"la base tiene \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"estos son \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columnas que tienen valores perdidos.\")\n",
    "         # retorna a un dataframe la informacion\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:/usil/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_202201_202203 = pd.read_csv(file_path+'08_target_202201_202203.csv')#\n",
    "convergente_202201_202204 = pd.read_csv(file_path+'07_convergente_202201_202204.csv')\n",
    "trafico_202201_202204 = pd.read_csv(file_path+'06_trafico_202201_202204.csv')#\n",
    "terminales_202201_202204 = pd.read_csv(file_path+'05_terminales_202201_202204.csv')#\n",
    "roaming_202201_202204 = pd.read_csv(file_path+'04_roaming_202201_202204.csv')\n",
    "perfil_digital_202201_202204 = pd.read_csv(file_path+'03_perfil_digital_202201_202204.csv')#\n",
    "adenda_202201_202204 = pd.read_csv(file_path+'02_adenda_202201_202204.csv')#\n",
    "suscriptora_202201_202204 = pd.read_csv(file_path+'01_suscriptora_202201_202204.csv')#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=target_202201_202203.copy()\n",
    "target['key_cliente']=target.PERIODO.astype(str)+'_'+target.nro_telefono_hash.astype(str)\n",
    "target.drop(['PERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "#target_202201_202203['TARGET']=target_202201_202203['TARGET'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# suscriptora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-01T00:00:00.000000000\n",
      "2022-03-01T00:00:00.000000000\n",
      "2022-04-01T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "suscriptora=suscriptora_202201_202204.copy()\n",
    "suscriptora['key_cliente']=suscriptora.NUMPERIODO.astype(str)+'_'+suscriptora.nro_telefono_hash.astype(str)\n",
    "suscriptora['key_documento']=suscriptora.NUMPERIODO.astype(str)+'_'+suscriptora.nro_documento_hash.astype(str)\n",
    "\n",
    "suscriptora=parse_month(suscriptora,['NUMPERIODO'],newcol=True)\n",
    "suscriptora['NUMPERIODO_parsed'] =pd.to_datetime(suscriptora['NUMPERIODO_parsed'] , format='%Y/%m/%d')\n",
    "suscriptora.shape\n",
    "\n",
    "#suscriptora.FECINGRESOCLIENTE.str.slice(0,7).value_counts().sort_index()[0:50]\n",
    "for mes in tqdm(suscriptora.NUMPERIODO_parsed.unique()):\n",
    "    print(mes)\n",
    "    suscriptora.loc[suscriptora['NUMPERIODO_parsed'] == mes,'NUMPERIODO_parsed']=pd.to_datetime(mes)+ relativedelta(months=1) - relativedelta(days=1)\n",
    "    \n",
    "suscriptora['FECINGRESOCLIENTE']=np.where(suscriptora['FECINGRESOCLIENTE'].isin(['0001-01-01 00:00:00'])\n",
    "                                           ,np.nan,suscriptora['FECINGRESOCLIENTE'])\n",
    "suscriptora['FECINGRESOCLIENTE'] = pd.to_datetime(suscriptora['FECINGRESOCLIENTE'] , format='%Y-%m-%d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:00<00:00, 78.21it/s]\n"
     ]
    }
   ],
   "source": [
    "datetime_fold=suscriptora[suscriptora['FECINGRESOCLIENTE']>pd.to_datetime('2022-04-30 00:00:00')]\n",
    "for mes in tqdm(datetime_fold['FECINGRESOCLIENTE'].unique()):\n",
    "    #print(mes)\n",
    "    suscriptora.loc[suscriptora['FECINGRESOCLIENTE'] == mes,'FECINGRESOCLIENTE']=np.nan\n",
    "\n",
    "suscriptora['tiempo_entel']=(suscriptora.NUMPERIODO_parsed-suscriptora.FECINGRESOCLIENTE).astype('<m8[M]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "suscriptora['aniomes_fic']=suscriptora.FECINGRESOCLIENTE.astype(str).str.slice(0,7)#.value_counts().sort_index()[240:]\n",
    "suscriptora['aniomes_fac']=suscriptora.FECACTIVACIONCONTRATO.astype(str).str.slice(0,7)#.value_counts().sort_index()[240:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMANDO LAS VARIABLES DE FECHAS A MESES\n",
    "#suscriptora.FECACTIVACIONCONTRATO.str.slice(0,7).value_counts().sort_index()[0:50]\n",
    "suscriptora['FECACTIVACIONCONTRATO']=np.where(suscriptora['FECACTIVACIONCONTRATO'].isin(['0001-01-01 00:00:00'])\n",
    "                                                                           ,np.nan,suscriptora['FECACTIVACIONCONTRATO'])\n",
    "suscriptora['FECACTIVACIONCONTRATO'] = pd.to_datetime(suscriptora['FECACTIVACIONCONTRATO'] , format='%Y-%m-%d')\n",
    "\n",
    "suscriptora['tiempo_contrato']=suscriptora.NUMPERIODO_parsed-suscriptora.FECACTIVACIONCONTRATO\n",
    "suscriptora['tiempo_contrato']=suscriptora['tiempo_contrato'].astype('<m8[M]')\n",
    "suscriptora['tiempo_contrato_ingreso']=suscriptora.FECACTIVACIONCONTRATO-suscriptora.FECINGRESOCLIENTE\n",
    "suscriptora['tiempo_contrato_ingreso']=suscriptora['tiempo_contrato_ingreso'].astype('<m8[M]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  4.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# nro de clientes por Tipo de adquisión\n",
    "suscriptora['TIPO_ADQ_CALC']=suscriptora.TIPO_ADQ\n",
    "for mes in tqdm(suscriptora['NUMPERIODO'].unique()):\n",
    "    NCLIENTES_TIPO_ADQUI=suscriptora.loc[(suscriptora.NUMPERIODO==mes)].TIPO_ADQ.value_counts()\n",
    "    suscriptora.loc[suscriptora.NUMPERIODO==mes,'TIPO_ADQ_CALC']=suscriptora.loc[suscriptora.NUMPERIODO==mes,'TIPO_ADQ'].map(NCLIENTES_TIPO_ADQUI)\n",
    "suscriptora['TIPO_ADQ_CALC']=suscriptora['TIPO_ADQ_CALC'].astype('int')\n",
    "\n",
    "# nro de clientes por Tipo de adquisión\n",
    "suscriptora['aniomes_fic_cal']=suscriptora.aniomes_fic\n",
    "for mes in tqdm(suscriptora['NUMPERIODO'].unique()):\n",
    "    nro_cli_aniomes_fic=suscriptora.loc[(suscriptora.NUMPERIODO==mes)].aniomes_fic.value_counts()\n",
    "    suscriptora.loc[suscriptora.NUMPERIODO==mes,'aniomes_fic_cal']=suscriptora.loc[suscriptora.NUMPERIODO==mes,'aniomes_fic'].map(nro_cli_aniomes_fic)\n",
    "suscriptora['aniomes_fic_cal']=suscriptora['aniomes_fic_cal'].astype('int')\n",
    "\n",
    "# nro de clientes por Tipo de adquisión\n",
    "suscriptora['aniomes_fac_cal']=suscriptora.aniomes_fac\n",
    "for mes in tqdm(suscriptora['NUMPERIODO'].unique()):\n",
    "    nro_cli_aniomes_fac=suscriptora.loc[(suscriptora.NUMPERIODO==mes)].aniomes_fac.value_counts()\n",
    "    suscriptora.loc[suscriptora.NUMPERIODO==mes,'aniomes_fac_cal']=suscriptora.loc[suscriptora.NUMPERIODO==mes,'aniomes_fac'].map(nro_cli_aniomes_fac)\n",
    "suscriptora['aniomes_fac_cal']=suscriptora['aniomes_fac_cal'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "230236it [3:09:13, 20.28it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_suscrip_features_single(doc,periodo):\n",
    "    q1 = suscriptora.query('nro_documento_hash==\"{}\" and NUMPERIODO=={}'.format(doc,periodo))\n",
    "    return q1\n",
    "\n",
    "usg, usgf = [],[]\n",
    "df1=suscriptora[['NUMPERIODO','nro_documento_hash']].drop_duplicates()\n",
    "for doc,periodo in tqdm(zip(df1['nro_documento_hash'],df1['NUMPERIODO'])):\n",
    "    #print(doc,periodo)\n",
    "    a = get_suscrip_features_single(doc,periodo)\n",
    "    b=a.groupby('TIPO_ADQ')['TIPO_ADQ'].transform('count').rename('tipo_adq_counts')\n",
    "    usg.append(pd.concat([a, b], axis=1))\n",
    "suscriptora=pd.concat(usg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNION DE LAS PRIMERAS TABLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION DE SUSCRIPTORA Y TARGET\n",
    "###########################################################\n",
    "suscriptora.drop(['FECINGRESOCLIENTE','FECACTIVACIONCONTRATO'\n",
    "                              ,'NUMPERIODO_parsed','aniomes_fic','aniomes_fac'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(suscriptora,target,on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(734544, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CALCULO DEL DESERTOR POR TELEFONO EN LA HISTORIA ROLLING\n",
    "desertor_hist= []  \n",
    "hist=df_full_hist[['NUMPERIODO','nro_telefono_hash']]\n",
    "hist_fugas=df_full_hist[(df_full_hist.TARGET==1)][['NUMPERIODO','nro_telefono_hash']] # fugas hist\n",
    "for mes in tqdm(hist['NUMPERIODO'].unique()):\n",
    "    partial=hist[hist['NUMPERIODO']==mes][['NUMPERIODO','nro_telefono_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist_fugas[(hist_fugas['nro_telefono_hash'].isin(partial['nro_telefono_hash'].unique())) \n",
    "            & (hist_fugas['NUMPERIODO']<mes)      \n",
    "                    ]\n",
    "    q1=q.groupby('nro_telefono_hash').count().add_suffix('_nro_salidas').fillna(0)\n",
    "    desertor_hist.append(\n",
    "     partial.merge(q1,on='nro_telefono_hash',how='left')\n",
    "    )\n",
    "desertor_hist = pd.concat(desertor_hist,sort=False).fillna(0)\n",
    "desertor_hist.drop_duplicates(inplace=True)\n",
    "desertor_hist['key_cliente']=desertor_hist.NUMPERIODO.astype(str)+'_'+desertor_hist.nro_telefono_hash.astype(str)\n",
    "# renombramos \n",
    "desertor_hist.rename(columns={'NUMPERIODO_nro_salidas':'renovo_hist'},inplace=True)\n",
    "###############################################3\n",
    "# agregamos a la tabla general\n",
    "desertor_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,desertor_hist,on=['key_cliente'],how='left')\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "suscriptora=pd.merge(suscriptora,df_full_hist[['key_cliente','renovo_hist']],on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:25<00:00,  6.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(734544, 49)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREANDO VARIABLES EN FUNCION AL TIP_ADQ\n",
    "suscrip = []\n",
    "for mes in tqdm(suscriptora['NUMPERIODO'].unique()):\n",
    "        partial = suscriptora[suscriptora['NUMPERIODO']==mes][['NUMPERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = suscriptora[(suscriptora['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                       & (suscriptora.NUMPERIODO==mes)]\n",
    "        \n",
    "        q1 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['nro_documento_hash'].count().unstack().add_prefix('count_d_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_entel'].mean().unstack().add_prefix('t_e_prom_d_').reset_index().fillna(0)\n",
    "        q3 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_entel'].min().unstack().add_prefix('t_e_min_d_').reset_index().fillna(0)\n",
    "        q4 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_entel'].max().unstack().add_prefix('t_e_max_d_').reset_index().fillna(0)\n",
    "        q5 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_entel'].std().unstack().add_prefix('t_e_std_d_').reset_index().fillna(0)\n",
    "\n",
    "        q6 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato'].mean().unstack().add_prefix('t_c_prom_d_').reset_index().fillna(0)\n",
    "        q7 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato'].min().unstack().add_prefix('t_c_min_d_').reset_index().fillna(0)\n",
    "        q8 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato'].max().unstack().add_prefix('t_c_max_d_').reset_index().fillna(0)\n",
    "        q9 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato'].std().unstack().add_prefix('t_c_std_d_').reset_index().fillna(0)\n",
    "\n",
    "        q10 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato_ingreso'].mean().unstack().add_prefix('t_c_i_prom_d_').reset_index().fillna(0)\n",
    "        q11 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato_ingreso'].min().unstack().add_prefix('t_c_i_min_d_').reset_index().fillna(0)\n",
    "        q12 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato_ingreso'].max().unstack().add_prefix('t_c_i_max_d_').reset_index().fillna(0)\n",
    "        q13 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['tiempo_contrato_ingreso'].std().unstack().add_prefix('t_c_i_std_d_').reset_index().fillna(0)\n",
    "        \n",
    "        q14 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['renovo_hist'].mean().unstack().add_prefix('t_r_mean_d_').reset_index().fillna(0)\n",
    "        q15 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['renovo_hist'].min().unstack().add_prefix('t_r_min_d_').reset_index().fillna(0)\n",
    "        q16 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['renovo_hist'].max().unstack().add_prefix('t_r_max_d_').reset_index().fillna(0)\n",
    "        q17 = q.groupby(['nro_documento_hash','TIPO_ADQ'])['renovo_hist'].std().unstack().add_prefix('t_r_std_d_').reset_index().fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "        suscrip.append(partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "                           .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "                       .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "                       .merge(q7,on='nro_documento_hash',how='left').merge(q8,on='nro_documento_hash',how='left')\n",
    "                       .merge(q9,on='nro_documento_hash',how='left').merge(q10,on='nro_documento_hash',how='left')\n",
    "                       .merge(q11,on='nro_documento_hash',how='left').merge(q12,on='nro_documento_hash',how='left')\n",
    "                       .merge(q13,on='nro_documento_hash',how='left').merge(q14,on='nro_documento_hash',how='left')\n",
    "                       .merge(q15,on='nro_documento_hash',how='left').merge(q16,on='nro_documento_hash',how='left')\n",
    "                       .merge(q17,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "suscrip = pd.concat(suscrip,sort=False)\n",
    "suscrip=suscrip.drop_duplicates()\n",
    "suscrip['key_documento']=suscrip.NUMPERIODO.astype(str)+'_'+suscrip.nro_documento_hash.astype(str)\n",
    "#############################################################3\n",
    "# UNION DE SUSCRIP\n",
    "suscrip.drop(['NUMPERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,suscrip,on=['key_documento'],how='left')\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.97s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(734544, 73)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREANDO VARIABLES \n",
    "suscrip2 = []\n",
    "suscrip_tmp=suscriptora[['NUMPERIODO','nro_documento_hash','tiempo_entel','tiempo_contrato','tiempo_contrato_ingreso','renovo_hist']]\n",
    "for mes in tqdm(suscrip_tmp['NUMPERIODO'].unique()):\n",
    "        partial = suscrip_tmp[suscrip_tmp['NUMPERIODO']==mes][['NUMPERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = suscrip_tmp[(suscrip_tmp['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                       & (suscrip_tmp.NUMPERIODO==mes)]\n",
    "        \n",
    "        q1 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).count().add_prefix('t_d_count_').reset_index().fillna(0)\n",
    "        q2 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).mean().add_prefix('t_d_prom_').reset_index().fillna(0)\n",
    "        q3 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).max().add_prefix('t_d_max_').reset_index().fillna(0)\n",
    "        q4 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).min().add_prefix('t_d_min_').reset_index().fillna(0)\n",
    "        q5 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).median().add_prefix('t_d_median_').reset_index().fillna(0)\n",
    "        q6 = q.drop('NUMPERIODO',axis=1).groupby(['nro_documento_hash']).std().add_prefix('t_d_std_').reset_index().fillna(0)\n",
    "\n",
    "        suscrip2.append(partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "                        .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "                        .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "suscrip2 = pd.concat(suscrip2,sort=False)\n",
    "suscrip2=suscrip2.drop_duplicates()\n",
    "suscrip2['key_documento']=suscrip2.NUMPERIODO.astype(str)+'_'+suscrip2.nro_documento_hash.astype(str)\n",
    "#############################################################3\n",
    "# UNION DE SUSCRIP\n",
    "suscrip2.drop(['NUMPERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,suscrip2,on=['key_documento'],how='left')\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.85s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(734544, 75)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FUGAS POR TIPO DE ADQUIZ - y telefono\n",
    "fugas = []\n",
    "for mes in tqdm(df_full_hist['NUMPERIODO'].unique()):\n",
    "        partial = df_full_hist[df_full_hist['NUMPERIODO']==mes][['key_cliente']] # verano de enero\n",
    "        q = df_full_hist[(df_full_hist['key_cliente'].isin(partial['key_cliente'].unique()))]\n",
    "        q1 = q.groupby(['key_cliente','TIPO_ADQ'])['renovo_hist'].sum().unstack().add_prefix('cont_fugas_').reset_index().fillna(0)\n",
    "        fugas.append(partial.merge(q1,on='key_cliente',how='left')\n",
    "                        )\n",
    "fugas = pd.concat(fugas,sort=False)\n",
    "fugas=fugas.drop_duplicates()\n",
    "df_full_hist=pd.merge(df_full_hist,fugas,on=['key_cliente'],how='left')\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tabla de digital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nro_telefono_hash: Código de identificación de la línea.\n",
    "#### GRUPO: Categoria de aplicaciones usadas por la línea.\n",
    "#### SCORECAT: Clasificación de que tan digital es el cliente categorizados por niveles.\n",
    "#### PERIODO: Periodo de cierre de mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfil_digital=perfil_digital_202201_202204.copy()\n",
    "perfil_digital['key_cliente']=perfil_digital.PERIODO.astype(str)+'_'+perfil_digital.nro_telefono_hash.astype(str)\n",
    "perfil_digital=pd.merge(perfil_digital,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')\n",
    "perfil_digital['key_documento']=perfil_digital.PERIODO.astype(str)+'_'+perfil_digital.nro_documento_hash.astype(str)\n",
    "\n",
    "dic = {'bajo': 1, 'medio': 2,'alto': 3, 'muy alto': 4}\n",
    "perfil_digital['scortCat_val']= perfil_digital['SCORECAT']\n",
    "perfil_digital['scortCat_val']=perfil_digital['SCORECAT'].map(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113808it [2:03:55, 16.21it/s]"
     ]
    }
   ],
   "source": [
    "def get_digital_features_single(doc,periodo):\n",
    "    q1 = perfil_digital.query('nro_documento_hash==\"{}\" and PERIODO=={}'.format(doc,periodo))\n",
    "    return q1\n",
    "\n",
    "usg, usgf = [],[]\n",
    "df1=perfil_digital[['PERIODO','nro_documento_hash']].drop_duplicates()\n",
    "for doc,periodo in tqdm(zip(df1['nro_documento_hash'],df1['PERIODO'])):\n",
    "    #print(doc,periodo)\n",
    "    a = get_digital_features_single(doc,periodo)\n",
    "    b=a.groupby('SCORECAT')['SCORECAT'].transform('count').rename('scorecat_counts')\n",
    "    usg.append(pd.concat([a, b], axis=1))\n",
    "perfil_digital=pd.concat(usg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nro de clientes por Tipo de categoria\n",
    "perfil_digital['SCORECAT_CALC']=perfil_digital.SCORECAT\n",
    "for mes in tqdm(perfil_digital['PERIODO'].unique()):\n",
    "    NCLIENTES_TIPO_CATE=perfil_digital.loc[(perfil_digital.PERIODO==mes)].SCORECAT.value_counts()\n",
    "    perfil_digital.loc[perfil_digital.PERIODO==mes,'SCORECAT_CALC']=perfil_digital.loc[perfil_digital.PERIODO==mes,'SCORECAT'].map(NCLIENTES_TIPO_CATE)\n",
    "perfil_digital['SCORECAT_CALC']=perfil_digital['SCORECAT_CALC'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMAION DE LA VARIABLE CADENA DE DATOS A COLUMNAS\n",
    "perfil_digital=pd.concat([perfil_digital,perfil_digital.GRUPO.str.get_dummies(sep = \"|\")], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMANDO TODAS LAS CATEGORIAS USADAS\n",
    "grupo_total=('grupo_1','grupo_2','grupo_3','grupo_4','grupo_5','grupo_6','grupo_7','grupo_8','grupo_9','grupo_10','grupo_11')\n",
    "perfil_digital['grupo_total']=perfil_digital.loc[:,grupo_total].sum(axis=1) # \n",
    "#perfil_digital.drop(['grupo_1','grupo_2','grupo_3','grupo_4','grupo_5','grupo_6','grupo_7','grupo_8','grupo_9','grupo_10','grupo_11'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# por telefono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfil_digital=pd.merge(perfil_digital,df_full_hist[['key_cliente','renovo_hist']],on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo telefono\n",
    "digital = []\n",
    "for mes in tqdm(perfil_digital['PERIODO'].unique()):\n",
    "        partial = perfil_digital[perfil_digital['PERIODO']==mes][['key_cliente']] # verano de enero\n",
    "        q = perfil_digital[(perfil_digital['key_cliente'].isin(partial['key_cliente'].unique()))]\n",
    "        q1 = q.groupby(['key_cliente','SCORECAT'])['grupo_total'].sum().unstack().add_prefix('sum_digi_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['key_cliente','SCORECAT'])['renovo_hist'].sum().unstack().add_prefix('sum_rc_').reset_index().fillna(0)\n",
    "\n",
    "        digital.append(partial.merge(q1,on='key_cliente',how='left').merge(q1,on='key_cliente',how='left')\n",
    "                        )\n",
    "digital = pd.concat(digital,sort=False)\n",
    "digital=digital.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORT DE CATEGORIAS USADAS ROLLING POR TIPO DE USUARIO TELEFONO\n",
    "# el max , min y prom de clasificacion que obtuvo en su historia\n",
    "scortCat_hist= []  \n",
    "hist=perfil_digital[['PERIODO','nro_telefono_hash','scortCat_val','grupo_total','renovo_hist']]\n",
    "for mes in tqdm(hist['PERIODO'].unique()):\n",
    "    partial=hist[hist['PERIODO']==mes][['PERIODO','nro_telefono_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_telefono_hash'].isin(partial['nro_telefono_hash'].unique())) \n",
    "            & (hist['PERIODO']<=mes)\n",
    "                    ]\n",
    "    \n",
    "    q1=q.drop('PERIODO',axis=1).groupby('nro_telefono_hash').max().add_suffix('_nro_max').fillna(0)\n",
    "    q2=q.drop('PERIODO',axis=1).groupby('nro_telefono_hash').min().add_suffix('_nro_min').fillna(0)\n",
    "    q3=q.drop('PERIODO',axis=1).groupby('nro_telefono_hash').mean().add_suffix('_nro_prom').fillna(0)\n",
    "    q4=q.drop('PERIODO',axis=1).groupby('nro_telefono_hash').sum().add_suffix('_nro_sum').fillna(0)\n",
    "\n",
    "    \n",
    "    scortCat_hist.append(\n",
    "     partial.merge(q1,on='nro_telefono_hash',how='left').merge(q2,on='nro_telefono_hash',how='left')\n",
    "        .merge(q3,on='nro_telefono_hash',how='left').merge(q4,on='nro_telefono_hash',how='left')\n",
    "    )\n",
    "scortCat_hist = pd.concat(scortCat_hist,sort=False).fillna(0)\n",
    "scortCat_hist.drop_duplicates(inplace=True)\n",
    "scortCat_hist['key_cliente']=scortCat_hist.PERIODO.astype(str)+'_'+scortCat_hist.nro_telefono_hash.astype(str)\n",
    "#desertor_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "digital_cli = []\n",
    "for mes in tqdm(perfil_digital['PERIODO'].unique()):\n",
    "        partial = perfil_digital[perfil_digital['PERIODO']==mes][['PERIODO','nro_telefono_hash']] # verano de enero\n",
    "        q = perfil_digital[(perfil_digital['nro_telefono_hash'].isin(partial['nro_telefono_hash'].unique()))\n",
    "                           & (perfil_digital.PERIODO<=mes)]\n",
    "        \n",
    "        q1 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].sum().unstack().add_prefix('sum_digi_cli_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].mean().unstack().add_prefix('mean_digi_cli_').reset_index().fillna(0)\n",
    "        q3 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].min().unstack().add_prefix('min_digi_cli_').reset_index().fillna(0)\n",
    "        q4 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].max().unstack().add_prefix('max_digi_cli_').reset_index().fillna(0)\n",
    "        q5 = q.groupby(['nro_telefono_hash','SCORECAT'])['grupo_total'].std().unstack().add_prefix('std_digi_cli_').reset_index().fillna(0)\n",
    "        \n",
    "        q6 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].sum().unstack().add_prefix('sum_digi_rc_').reset_index().fillna(0)\n",
    "        q7 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].mean().unstack().add_prefix('mean_digi_rc_').reset_index().fillna(0)\n",
    "        q8 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].min().unstack().add_prefix('min_digi_rc_').reset_index().fillna(0)\n",
    "        q9 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].max().unstack().add_prefix('max_digi_rc_').reset_index().fillna(0)\n",
    "        q10 = q.groupby(['nro_telefono_hash','SCORECAT'])['renovo_hist'].std().unstack().add_prefix('std_digi_rc_').reset_index().fillna(0)\n",
    "        digital_cli.append(partial.merge(q1,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q2,on='nro_telefono_hash',how='left').merge(q3,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q4,on='nro_telefono_hash',how='left').merge(q5,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q6,on='nro_telefono_hash',how='left').merge(q7,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q8,on='nro_telefono_hash',how='left').merge(q9,on='nro_telefono_hash',how='left')\n",
    "                           .merge(q10,on='nro_telefono_hash',how='left')\n",
    "                        )\n",
    "digital_cli = pd.concat(digital_cli,sort=False)\n",
    "digital_cli=digital_cli.drop_duplicates()\n",
    "digital_cli['key_cliente']=digital_cli.PERIODO.astype(str)+'_'+digital_cli.nro_telefono_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "digital_doc = []\n",
    "for mes in tqdm(perfil_digital['PERIODO'].unique()):\n",
    "        partial = perfil_digital[perfil_digital['PERIODO']==mes][['PERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = perfil_digital[(perfil_digital['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                           & (perfil_digital.PERIODO==mes)]\n",
    "        q_ = perfil_digital[(perfil_digital['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                           & (perfil_digital.PERIODO<=mes)]\n",
    "        \n",
    "        q1 = q.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].sum().unstack().add_prefix('sum_digi_doc_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].mean().unstack().add_prefix('mean_digi_doc_').reset_index().fillna(0)\n",
    "        q3 = q_.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].sum().unstack().add_prefix('sum_digi_doc2_').reset_index().fillna(0)\n",
    "        q4 = q_.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].mean().unstack().add_prefix('mean_digi_doc2_').reset_index().fillna(0)\n",
    "        q5 = q.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].min().unstack().add_prefix('min_digi_doc_').reset_index().fillna(0)\n",
    "        q6 = q_.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].min().unstack().add_prefix('min_digi_doc2_').reset_index().fillna(0)\n",
    "        q7 = q.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].max().unstack().add_prefix('max_digi_doc_').reset_index().fillna(0)\n",
    "        q8 = q_.groupby(['nro_documento_hash','SCORECAT'])['grupo_total'].max().unstack().add_prefix('max_digi_doc2_').reset_index().fillna(0)\n",
    "        \n",
    "        q9 = q.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].sum().unstack().add_prefix('sum_digi_rc_').reset_index().fillna(0)\n",
    "        q10 = q.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].mean().unstack().add_prefix('mean_digi_rc_').reset_index().fillna(0)\n",
    "        q11 = q_.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].sum().unstack().add_prefix('sum_digi_rc_').reset_index().fillna(0)\n",
    "        q12 = q_.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].mean().unstack().add_prefix('mean_digi_rc_').reset_index().fillna(0)\n",
    "        q13 = q.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].min().unstack().add_prefix('min_digi_rc_').reset_index().fillna(0)\n",
    "        q14 = q_.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].min().unstack().add_prefix('min_digi_rc_').reset_index().fillna(0)\n",
    "        q15 = q.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].max().unstack().add_prefix('max_digi_rc_').reset_index().fillna(0)\n",
    "        q16 = q_.groupby(['nro_documento_hash','SCORECAT'])['renovo_hist'].max().unstack().add_prefix('max_digi_rc_').reset_index().fillna(0)\n",
    "\n",
    "        digital_doc.append(partial.merge(q1,on='nro_documento_hash',how='left')\n",
    "                           .merge(q2,on='nro_documento_hash',how='left').merge(q3,on='nro_documento_hash',how='left')\n",
    "                           .merge(q4,on='nro_documento_hash',how='left').merge(q5,on='nro_documento_hash',how='left')\n",
    "                           .merge(q6,on='nro_documento_hash',how='left').merge(q7,on='nro_documento_hash',how='left')\n",
    "                           .merge(q8,on='nro_documento_hash',how='left').merge(q9,on='nro_documento_hash',how='left')\n",
    "                           .merge(q10,on='nro_documento_hash',how='left').merge(q11,on='nro_documento_hash',how='left')\n",
    "                           .merge(q12,on='nro_documento_hash',how='left').merge(q13,on='nro_documento_hash',how='left')\n",
    "                           .merge(q14,on='nro_documento_hash',how='left').merge(q15,on='nro_documento_hash',how='left')\n",
    "                           .merge(q16,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "digital_doc = pd.concat(digital_doc,sort=False)\n",
    "digital_doc=digital_doc.drop_duplicates()\n",
    "digital_doc['key_documento']=digital_doc.PERIODO.astype(str)+'_'+digital_doc.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scortCat_doc_hist= []  \n",
    "hist=perfil_digital[['PERIODO','nro_documento_hash','scortCat_val','grupo_total','renovo_hist']]\n",
    "for mes in tqdm(hist['PERIODO'].unique()):\n",
    "    partial=hist[hist['PERIODO']==mes][['PERIODO','nro_documento_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['PERIODO']<=mes)  \n",
    "            ]\n",
    "    q_ = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['PERIODO']==mes) \n",
    "                    ]\n",
    "    q1=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_snro_max').fillna(0)\n",
    "    q2=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_snro_min').fillna(0)\n",
    "    q3=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_snro_prom').fillna(0)\n",
    "    q4=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_snro_sum').fillna(0)\n",
    "    q5=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_snro_std').fillna(0)\n",
    "\n",
    "       \n",
    "    q6=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_snro_max_2').fillna(0)\n",
    "    q7=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_snro_min_2').fillna(0)\n",
    "    q8=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_snro_prom_2').fillna(0)\n",
    "    q9=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_snro_sum_2').fillna(0)\n",
    "    q10=q_.drop('PERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_snro_std_2').fillna(0)\n",
    "\n",
    "\n",
    "    \n",
    "    scortCat_doc_hist.append(\n",
    "     partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "        .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "        .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "        .merge(q7,on='nro_documento_hash',how='left').merge(q8,on='nro_documento_hash',how='left')\n",
    "        .merge(q9,on='nro_documento_hash',how='left').merge(q10,on='nro_documento_hash',how='left')\n",
    "    )\n",
    "scortCat_doc_hist = pd.concat(scortCat_doc_hist,sort=False).fillna(0)\n",
    "scortCat_doc_hist.drop_duplicates(inplace=True)\n",
    "scortCat_doc_hist['key_documento']=scortCat_doc_hist.PERIODO.astype(str)+'_'+scortCat_doc_hist.nro_documento_hash.astype(str)\n",
    "#desertor_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION DE LAS TABLAS CREADAS DE DIGITAL\n",
    "perfil_digital.drop(['PERIODO','nro_telefono_hash','nro_documento_hash','key_documento','GRUPO','SCORECAT'],axis=1,inplace=True)\n",
    "digital_doc.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "digital_cli.drop(['PERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "scortCat_hist.drop(['PERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "scortCat_doc_hist.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "df_full_hist=pd.merge(df_full_hist,perfil_digital,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,digital,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,digital_doc,on=['key_documento'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,digital_cli,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,scortCat_hist,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,scortCat_doc_hist,on=['key_documento'],how='left')\n",
    "\n",
    "df_full_hist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADENDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VCHMESADENDA: Meses transcurridos desde el inicio de la adenda.\n",
    "##### VCHPENALIDAD: Monto de penalidad vigente a la fecha en caso de desactivación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adenda=adenda_202201_202204.copy()\n",
    "adenda['key_cliente']=adenda.NUMPERIODO.astype(str)+'_'+adenda.nro_telefono_hash.astype(str)\n",
    "adenda=pd.merge(adenda,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')\n",
    "adenda['key_documento']=adenda.NUMPERIODO.astype(str)+'_'+adenda.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adenda_features_single(key_doc,mes_adenda,penalidad,adenda):\n",
    "    q1 = adenda.query('key_documento==\"{}\" '.format(key_doc))\n",
    "    return (mes_adenda < q1['VCHMESADENDA']).mean(), (penalidad < q1['VCHPENALIDAD']).mean()\n",
    "\n",
    "def get_adenda_features(df,adenda):\n",
    "    # No es la forma mas rápida, pero hace el trabajo\n",
    "    usg, usgf = [],[]\n",
    "    adenda['VCHMESADENDA'] = adenda['VCHMESADENDA'].fillna(-999)\n",
    "    adenda['VCHPENALIDAD'] = adenda['VCHPENALIDAD'].fillna(-999)\n",
    "\n",
    "    for key_doc,mes_adenda,penalidad in tqdm(zip(adenda['key_documento'],adenda['VCHMESADENDA'],adenda['VCHPENALIDAD'])):\n",
    "        #print(key_doc,mes_adenda,penalidad)\n",
    "        a,b = get_adenda_features_single(key_doc,mes_adenda,penalidad,adenda)\n",
    "        usg.append(a)\n",
    "        usgf.append(b)\n",
    "    adenda['rel_use'] = usg\n",
    "    adenda['rel_use_full'] = usgf\n",
    "    \n",
    "    adenda['key_documento_count'] = np.nan\n",
    "    for m in tqdm(adenda['key_documento'].unique()):\n",
    "        adenda['key_documento_count'] = np.where(adenda['key_documento']==m,sum(adenda['key_documento']==m),adenda['key_documento_count'])\n",
    "        \n",
    "    adenda['VCHMESADENDA'] = adenda['VCHMESADENDA'].replace(-999,np.nan)\n",
    "    adenda['VCHPENALIDAD'] = adenda['VCHPENALIDAD'].replace(-999,np.nan)\n",
    "  \n",
    "    agg = {'key_documento' : ['count'],\n",
    "                   'rel_use' : ['mean'],\n",
    "           'rel_use_full' : ['mean']}\n",
    "    \n",
    "    adendap = adenda.groupby('key_documento').agg(agg).reset_index()\n",
    "    adendap.columns = ['key_documento']+[a+'_'+b for a,b in adendap.columns[1:]]\n",
    "\n",
    "    return df.merge(adendap,on='key_documento',how='left')\n",
    "df_full_hist = get_adenda_features(df_full_hist, adenda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adenda1=adenda[adenda.key_documento.isin(['202201_7759b41a7d1df4b2d8f721577abc24a48d95dc5f01fbab5937e9b3f6bfe84fc6'\n",
    "                                  ,'202202_f3d5b2a430a3fd220d08e43875ab977aaacbf4e7b167c9993804e5d5e15caafc'\n",
    "                                         ,'202201_b8627a8bd1b8e4e6f05fa2adb87e51cea0ffe232ae0b117ea6604b89f123fa4a'\n",
    "                                         ,'202201_bcddf613461efae35b7001611e394258b231b88cbdb61c03123634ce48682528'])]\n",
    "get_adenda_features(df_full_hist,adenda1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estadisticas a nivel de documento\n",
    "\n",
    "adenda_doc_hist= []  \n",
    "hist=adenda[['NUMPERIODO','nro_documento_hash','VCHMESADENDA','VCHPENALIDAD']]\n",
    "for mes in tqdm(hist['NUMPERIODO'].unique()):\n",
    "    partial=hist[hist['NUMPERIODO']==mes][['NUMPERIODO','nro_documento_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']<=mes)      \n",
    "                    ]\n",
    "    q_ = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']==mes)\n",
    "             ]\n",
    "    q1=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_nro_max').fillna(0)\n",
    "    q2=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_nro_min').fillna(0)\n",
    "    q3=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_nro_prom').fillna(0)\n",
    "    q4=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_nro_sum').fillna(0)\n",
    "    q5=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_nro_std').fillna(0)\n",
    "\n",
    "\n",
    "    \n",
    "    q6=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_nro_max2').fillna(0)\n",
    "    q7=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_nro_min2').fillna(0)\n",
    "    q8=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_nro_prom2').fillna(0)\n",
    "    q9=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_nro_sum2').fillna(0)\n",
    "    q10=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_nro_std2').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "    adenda_doc_hist.append(\n",
    "     partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "        .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "        .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "        .merge(q7,on='nro_documento_hash',how='left').merge(q8,on='nro_documento_hash',how='left')\n",
    "        .merge(q9,on='nro_documento_hash',how='left').merge(q10,on='nro_documento_hash',how='left')\n",
    "    )\n",
    "adenda_doc_hist = pd.concat(adenda_doc_hist,sort=False).fillna(0)\n",
    "adenda_doc_hist.drop_duplicates(inplace=True)\n",
    "adenda_doc_hist['key_documento']=adenda_doc_hist.NUMPERIODO.astype(str)+'_'+adenda_doc_hist.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION DE LAS TABLAS CREADAS DE adendas\n",
    "adenda.drop(['NUMPERIODO','nro_telefono_hash','nro_documento_hash','key_documento'],axis=1,inplace=True)\n",
    "#adenda_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "adenda_doc_hist.drop(['NUMPERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "\n",
    "#df_full_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,adenda,on=['key_cliente'],how='left')\n",
    "#df_full_hist=pd.merge(df_full_hist,adenda_hist,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,adenda_doc_hist,on=['key_documento'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full_hist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAFICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafico=trafico_202201_202204.copy()\n",
    "trafico['key_cliente']=trafico.NUMPERIODO.astype(str)+'_'+trafico.nro_telefono_hash.astype(str)\n",
    "trafico=pd.merge(trafico,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMANDO TODAS los traficos \n",
    "trafico_subtotal=('trafico_app_1','trafico_app_2' ,'trafico_app_7','trafico_app_8'\n",
    "             ,'trafico_app_3','trafico_app_4','trafico_app_5','trafico_app_6','trafico_app_9')\n",
    "trafico['trafico_subtotal']=trafico.loc[:,trafico_subtotal].sum(axis=1) \n",
    "trafico['peso_traf']=trafico['trafico_subtotal']/trafico['trafico_total']\n",
    "\n",
    "mins_flujo=('mins_flujo_1','mins_flujo_2')\n",
    "trafico['mins_flujo_total']=trafico.loc[:,mins_flujo].sum(axis=1) \n",
    "trafico['peso_flujo1']=trafico['mins_flujo_1']/trafico['mins_flujo_total']\n",
    "trafico['peso_flujo2']=trafico['mins_flujo_2']/trafico['mins_flujo_total']\n",
    "\n",
    "#trafico.drop(['trafico_app_1','trafico_app_2' ,'trafico_app_7','trafico_app_8'\n",
    "             #,'trafico_app_3','trafico_app_4','trafico_app_5','trafico_app_6','trafico_app_9'\n",
    "            # ,'mins_flujo_1','mins_flujo_2'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "traf_doc_hist= []  \n",
    "hist=trafico[['NUMPERIODO','nro_documento_hash','trafico_subtotal','mins_flujo_total','trafico_total']]\n",
    "for mes in tqdm(hist['NUMPERIODO'].unique()):\n",
    "    partial=hist[hist['NUMPERIODO']==mes][['NUMPERIODO','nro_documento_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']<=mes)      \n",
    "                    ]\n",
    "    q_ = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']==mes)      \n",
    "                    ]\n",
    "    q1=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_tr_prom').fillna(0)\n",
    "    q2=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_tr_sum').fillna(0)\n",
    "    q3=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_tr_min').fillna(0)\n",
    "    q4=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_tr_max').fillna(0)\n",
    "    q5=q.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_tr_std').fillna(0)\n",
    "\n",
    "    q6=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').mean().add_suffix('_doc_tr_prom2').fillna(0)\n",
    "    q7=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').sum().add_suffix('_doc_tr_sum2').fillna(0)\n",
    "    q8=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').min().add_suffix('_doc_tr_min2').fillna(0)\n",
    "    q9=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').max().add_suffix('_doc_tr_max2').fillna(0)\n",
    "    q10=q_.drop('NUMPERIODO',axis=1).groupby('nro_documento_hash').std().add_suffix('_doc_tr_std2').fillna(0)\n",
    "\n",
    "\n",
    "    traf_doc_hist.append(\n",
    "     partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "        .merge(q3,on='nro_documento_hash',how='left').merge(q4,on='nro_documento_hash',how='left')\n",
    "        .merge(q5,on='nro_documento_hash',how='left').merge(q6,on='nro_documento_hash',how='left')\n",
    "        .merge(q7,on='nro_documento_hash',how='left').merge(q8,on='nro_documento_hash',how='left')\n",
    "        .merge(q9,on='nro_documento_hash',how='left').merge(q10,on='nro_documento_hash',how='left')\n",
    "    )\n",
    "traf_doc_hist = pd.concat(traf_doc_hist,sort=False).fillna(0)\n",
    "traf_doc_hist.drop_duplicates(inplace=True)\n",
    "traf_doc_hist['key_documento']=traf_doc_hist.NUMPERIODO.astype(str)+'_'+traf_doc_hist.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "traf_tel_hist= []  \n",
    "hist=trafico[['NUMPERIODO','nro_telefono_hash','trafico_subtotal','mins_flujo_total','trafico_total']]\n",
    "for mes in tqdm(hist['NUMPERIODO'].unique()):\n",
    "    partial=hist[hist['NUMPERIODO']==mes][['NUMPERIODO','nro_telefono_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_telefono_hash'].isin(partial['nro_telefono_hash'].unique())) \n",
    "            & (hist['NUMPERIODO']<=mes)      \n",
    "                    ]\n",
    "    q1=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').mean().add_suffix('_tel_tr_prom').fillna(0)\n",
    "    q2=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').sum().add_suffix('_tel_tr_sum').fillna(0)\n",
    "    q3=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').min().add_suffix('_tel_tr_min').fillna(0)\n",
    "    q4=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').max().add_suffix('_tel_tr_max').fillna(0)\n",
    "    q5=q.drop('NUMPERIODO',axis=1).groupby('nro_telefono_hash').std().add_suffix('_tel_tr_std').fillna(0)\n",
    "\n",
    "\n",
    "    traf_tel_hist.append(\n",
    "     partial.merge(q1,on='nro_telefono_hash',how='left').merge(q2,on='nro_telefono_hash',how='left')\n",
    "        .merge(q3,on='nro_telefono_hash',how='left').merge(q4,on='nro_telefono_hash',how='left')\n",
    "        .merge(q5,on='nro_telefono_hash',how='left')\n",
    "    )\n",
    "traf_tel_hist = pd.concat(traf_tel_hist,sort=False).fillna(0)\n",
    "traf_tel_hist.drop_duplicates(inplace=True)\n",
    "traf_tel_hist['key_cliente']=traf_tel_hist.NUMPERIODO.astype(str)+'_'+traf_tel_hist.nro_telefono_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traf_tel_hist.drop(['NUMPERIODO','nro_telefono_hash'],axis=1,inplace=True)\n",
    "traf_doc_hist.drop(['NUMPERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "\n",
    "df_full_hist=pd.merge(df_full_hist,traf_tel_hist,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,traf_doc_hist,on=['key_documento'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafico.drop(['NUMPERIODO','nro_telefono_hash','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,trafico,on=['key_cliente'],how='left')\n",
    "print(df_full_hist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# terminales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_low_classes(lst,thresh,string):\n",
    "    c2r = lst.value_counts()[lst.value_counts()<thresh].index\n",
    "    return lst.replace({x:string for x in c2r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminales=terminales_202201_202204.copy()\n",
    "terminales['key_cliente']=terminales.PERIODO.astype(str)+'_'+terminales.nro_telefono_hash.astype(str)\n",
    "terminales=parse_month(terminales,['PERIODO'],newcol=True)\n",
    "#terminales.drop(['PERIODO','nro_telefono_hash'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminales['LANZAMIENTO'] = pd.to_datetime(terminales['LANZAMIENTO'] , format='%Y-%m-%d')\n",
    "datetime_fold=terminales[terminales['LANZAMIENTO']>pd.to_datetime('2022-04-30 00:00:00')]\n",
    "for mes in tqdm(datetime_fold['LANZAMIENTO'].unique()):\n",
    "    #print(mes)\n",
    "    terminales.loc[terminales['LANZAMIENTO'] == mes,'LANZAMIENTO']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminales['aniomes_lanz']=terminales.LANZAMIENTO.astype(str).str.slice(0,4)\n",
    "# nro de clientes por Tipo de adquisión\n",
    "terminales['aniomes_lanz_cal']=terminales.aniomes_lanz\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    nro_cli_aniomes_lanz=terminales.loc[(terminales.PERIODO==mes)].aniomes_lanz.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'aniomes_lanz_cal']=terminales.loc[terminales.PERIODO==mes,'aniomes_lanz'].map(nro_cli_aniomes_lanz)\n",
    "terminales['aniomes_lanz_cal']=terminales['aniomes_lanz_cal'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformacion para optener la variable de tiempo de lanzamiento\n",
    "terminales['lanzamiento_equipo']=(terminales.PERIODO_parsed-terminales.LANZAMIENTO).astype('<m8[M]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transfomracion de las variables categoricas\n",
    "terminales['MARCA'] = replace_low_classes(terminales['MARCA'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "terminales['MODELO'] = replace_low_classes(terminales['MODELO'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "terminales['NUEVA_GAMMA'] = replace_low_classes(terminales['NUEVA_GAMMA'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "terminales['OS'] = replace_low_classes(terminales['OS'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "terminales['DEVICE_TYPE'] = replace_low_classes(terminales['DEVICE_TYPE'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "\n",
    "terminales['MARCA']=terminales['MARCA'].astype('category')\n",
    "terminales['MODELO']=terminales['MODELO'].astype('category')\n",
    "terminales['NUEVA_GAMMA']=terminales['NUEVA_GAMMA'].astype('category')\n",
    "terminales['OS']=terminales['OS'].astype('category')\n",
    "terminales['DEVICE_TYPE']=terminales['DEVICE_TYPE'].astype('category')\n",
    "\n",
    "terminales['MARCA']= terminales['MARCA'].cat.codes\n",
    "terminales['MODELO']= terminales['MODELO'].cat.codes\n",
    "terminales['NUEVA_GAMMA']= terminales['NUEVA_GAMMA'].cat.codes\n",
    "terminales['OS']= terminales['OS'].cat.codes\n",
    "terminales['DEVICE_TYPE']= terminales['DEVICE_TYPE'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nro de clientes por Tipo de scort de categoria\n",
    "terminales['MARCA_CALC']=terminales.MARCA\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_MARCA=terminales.loc[(terminales.PERIODO==mes)].MARCA.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'MARCA_CALC']=terminales.loc[terminales.PERIODO==mes,'MARCA'].map(NCLIENTES_MARCA)\n",
    "terminales['MARCA_CALC']=terminales['MARCA_CALC'].astype('int')\n",
    "\n",
    "terminales['MODELO_CALC']=terminales.MODELO\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_MODELO=terminales.loc[(terminales.PERIODO==mes)].MODELO.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'MODELO_CALC']=terminales.loc[terminales.PERIODO==mes,'MODELO'].map(NCLIENTES_MODELO)\n",
    "terminales['MODELO_CALC']=terminales['MODELO_CALC'].astype('int')\n",
    "\n",
    "terminales['NUEVA_GAMMA_CALC']=terminales.NUEVA_GAMMA\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_NUEVA_GAMMA=terminales.loc[(terminales.PERIODO==mes)].NUEVA_GAMMA.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'NUEVA_GAMMA_CALC']=terminales.loc[terminales.PERIODO==mes,'NUEVA_GAMMA'].map(NCLIENTES_NUEVA_GAMMA)\n",
    "terminales['NUEVA_GAMMA_CALC']=terminales['NUEVA_GAMMA_CALC'].astype('int')\n",
    "\n",
    "terminales['OS_CALC']=terminales.OS\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_OS=terminales.loc[(terminales.PERIODO==mes)].OS.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'OS_CALC']=terminales.loc[terminales.PERIODO==mes,'OS'].map(NCLIENTES_OS)\n",
    "terminales['OS_CALC']=terminales['OS_CALC'].astype('int')\n",
    "\n",
    "terminales['DEVICE_TYPE_CALC']=terminales.DEVICE_TYPE\n",
    "for mes in tqdm(terminales['PERIODO'].unique()):\n",
    "    NCLIENTES_DEVICE_TYPE=terminales.loc[(terminales.PERIODO==mes)].DEVICE_TYPE.value_counts()\n",
    "    terminales.loc[terminales.PERIODO==mes,'DEVICE_TYPE_CALC']=terminales.loc[terminales.PERIODO==mes,'DEVICE_TYPE'].map(NCLIENTES_DEVICE_TYPE)\n",
    "terminales['DEVICE_TYPE_CALC']=terminales['DEVICE_TYPE_CALC'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminales=pd.merge(terminales,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_features_single(doc,periodo):\n",
    "    q1 = terminales.query('nro_documento_hash==\"{}\" and PERIODO=={}'.format(doc,periodo))\n",
    "    return q1\n",
    "\n",
    "usg, usgf = [],[]\n",
    "df1=terminales[['PERIODO','nro_documento_hash']].drop_duplicates()\n",
    "for doc,periodo in tqdm(zip(df1['nro_documento_hash'],df1['PERIODO'])):\n",
    "    #print(doc,periodo)\n",
    "    a = get_term_features_single(doc,periodo)\n",
    "    b=a.groupby('MARCA')['MARCA'].transform('count').rename('marca_counts')\n",
    "    c=a.groupby('MODELO')['MODELO'].transform('count').rename('modelo_counts')\n",
    "    d=a.groupby('NUEVA_GAMMA')['NUEVA_GAMMA'].transform('count').rename('ngama_counts')\n",
    "    e=a.groupby('OS')['OS'].transform('count').rename('os_counts')\n",
    "    f=a.groupby('DEVICE_TYPE')['DEVICE_TYPE'].transform('count').rename('device_counts')\n",
    "    usg.append(pd.concat([a, b, c, d, e, f], axis=1))\n",
    "terminales=pd.concat(usg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_term_features_single(doc,periodo):\n",
    "    q1 = terminales.query('nro_documento_hash==\"{}\" and PERIODO=={}'.format(doc,periodo))\n",
    "    return q1\n",
    "\n",
    "usg, usgf = [],[]\n",
    "terminales1=terminales[['PERIODO','nro_documento_hash']].drop_duplicates()\n",
    "for doc,periodo in tqdm(zip(terminales1['nro_documento_hash'],terminales1['PERIODO'])):\n",
    "    #print(doc,periodo)\n",
    "    a = get_term_features_single(doc,periodo)\n",
    "    b=a.groupby('MARCA')['MARCA'].transform('count')\n",
    "    usg.append(pd.concat([a, b], axis=1))\n",
    "c=pd.concat(usg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "terminales_doc_1 = []\n",
    "data_term=terminales[['nro_documento_hash','PERIODO','lanzamiento_equipo']]\n",
    "for mes in tqdm(data_term['PERIODO'].unique()):\n",
    "        partial = data_term[data_term['PERIODO']==mes][['PERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = data_term[(data_term['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                     & (data_term['PERIODO']==mes)]\n",
    "        \n",
    "        q1 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).mean().add_prefix('doc_ter_prom_').reset_index().fillna(0)\n",
    "        q2 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).min().add_prefix('doc_ter_min_').reset_index().fillna(0)\n",
    "        q3 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).max().add_prefix('doc_ter_max_').reset_index().fillna(0)\n",
    "        q4 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).std().add_prefix('doc_ter_std_').reset_index().fillna(0)\n",
    "\n",
    "        \n",
    "\n",
    "        terminales_doc_1.append(partial.merge(q1,on='nro_documento_hash',how='left')\n",
    "                                .merge(q2,on='nro_documento_hash',how='left').merge(q3,on='nro_documento_hash',how='left')\n",
    "                                .merge(q4,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "terminales_doc_1 = pd.concat(terminales_doc_1,sort=False)\n",
    "terminales_doc_1=terminales_doc_1.drop_duplicates()\n",
    "terminales_doc_1['key_documento']=terminales_doc_1.PERIODO.astype(str)+'_'+terminales_doc_1.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminales_doc= []  \n",
    "hist=terminales[['PERIODO','nro_documento_hash','MARCA','MODELO','NUEVA_GAMMA','OS','DEVICE_TYPE']]\n",
    "for mes in tqdm(hist['PERIODO'].unique()):\n",
    "    partial=hist[hist['PERIODO']==mes][['PERIODO','nro_documento_hash']]\n",
    "    partial=partial.drop_duplicates()\n",
    "    q = hist[(hist['nro_documento_hash'].isin(partial['nro_documento_hash'].unique())) \n",
    "            & (hist['PERIODO']==mes)      \n",
    "                    ]\n",
    "    q1=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').nunique().add_suffix('_nunique_prod_').fillna(0)\n",
    "    q2=q.drop('PERIODO',axis=1).groupby('nro_documento_hash').count().add_suffix('_count_prod_').fillna(0)\n",
    "\n",
    "\n",
    "    terminales_doc.append(\n",
    "     partial.merge(q1,on='nro_documento_hash',how='left').merge(q2,on='nro_documento_hash',how='left')\n",
    "       \n",
    "    )\n",
    "terminales_doc = pd.concat(terminales_doc,sort=False).fillna(0)\n",
    "terminales_doc.drop_duplicates(inplace=True)\n",
    "terminales_doc['key_documento']=terminales_doc.PERIODO.astype(str)+'_'+terminales_doc.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################33\n",
    "terminales.drop(['PERIODO','nro_telefono_hash','LANZAMIENTO','PERIODO_parsed','nro_documento_hash','aniomes_lanz'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,terminales,on=['key_cliente'],how='left')\n",
    "print(df_full_hist.shape)\n",
    "\n",
    "terminales_doc.drop(['PERIODO','nro_documento_hash','MODELO_count_prod_','NUEVA_GAMMA_count_prod_','OS_count_prod_','DEVICE_TYPE_count_prod_'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,terminales_doc,on=['key_documento'],how='left')\n",
    "\n",
    "terminales_doc_1.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,terminales_doc_1,on=['key_documento'],how='left')\n",
    "\n",
    "print(df_full_hist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convergente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergente=convergente_202201_202204.copy()\n",
    "convergente['key_documento']=convergente.PERIODO.astype(str)+'_'+convergente.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiene_prod_total=('TIENE_PROD_1','TIENE_PROD_2','TIENE_PROD_3')\n",
    "convergente['tiene_prod_total']=convergente.loc[:,tiene_prod_total].sum(axis=1) \n",
    "\n",
    "convergente['peso_PROD_1']=convergente['TIENE_PROD_1']/convergente['tiene_prod_total']\n",
    "convergente['peso_PROD_2']=convergente['TIENE_PROD_2']/convergente['tiene_prod_total']\n",
    "convergente['peso_PROD_3']=convergente['TIENE_PROD_3']/convergente['tiene_prod_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergente['GIRO'] = replace_low_classes(convergente['GIRO'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "convergente['SUBGIRO'] = replace_low_classes(convergente['SUBGIRO'],thresh=500,string='Otros').values # 7 =7 a más\n",
    "\n",
    "convergente['GIRO']=convergente['GIRO'].astype('category')\n",
    "convergente['SUBGIRO']=convergente['SUBGIRO'].astype('category')\n",
    "\n",
    "convergente['GIRO']= convergente['GIRO'].cat.codes\n",
    "convergente['SUBGIRO']= convergente['SUBGIRO'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nro de clientes por Tipo de scort de categoria\n",
    "convergente['GIRO_CALC']=convergente.GIRO\n",
    "for mes in tqdm(convergente['PERIODO'].unique()):\n",
    "    NCLIENTES_GIRO=convergente.loc[(convergente.PERIODO==mes)].GIRO.value_counts()\n",
    "    convergente.loc[convergente.PERIODO==mes,'GIRO_CALC']=convergente.loc[convergente.PERIODO==mes,'GIRO'].map(NCLIENTES_GIRO)\n",
    "convergente['GIRO_CALC']=convergente['GIRO_CALC'].astype('int')\n",
    "\n",
    "convergente['SUBGIRO_CALC']=convergente.SUBGIRO\n",
    "for mes in tqdm(convergente['PERIODO'].unique()):\n",
    "    NCLIENTES_SUBGIRO=convergente.loc[(convergente.PERIODO==mes)].SUBGIRO.value_counts()\n",
    "    convergente.loc[convergente.PERIODO==mes,'SUBGIRO_CALC']=convergente.loc[convergente.PERIODO==mes,'SUBGIRO'].map(NCLIENTES_SUBGIRO)\n",
    "convergente['SUBGIRO_CALC']=convergente['SUBGIRO_CALC'].astype('int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "convergente_hist = []\n",
    "data_conv=convergente[['nro_documento_hash','PERIODO','tiene_prod_total']]\n",
    "for mes in tqdm(data_conv['PERIODO'].unique()):\n",
    "        partial = data_conv[data_conv['PERIODO']==mes][['PERIODO','nro_documento_hash']] # verano de enero\n",
    "        q = data_conv[(data_conv['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                     & (data_conv['PERIODO']<=mes)]\n",
    "        q_ = data_conv[(data_conv['nro_documento_hash'].isin(partial['nro_documento_hash'].unique()))\n",
    "                     & (data_conv['PERIODO']==mes)]\n",
    "        \n",
    "        q1 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).sum().add_prefix('doc_conv_prom_').reset_index().fillna(0)\n",
    "        q2 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).sum().add_prefix('doc_conv_prom2_').reset_index().fillna(0)\n",
    "        q3 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).mean().add_prefix('doc_conv_mean_').reset_index().fillna(0)\n",
    "        q4 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).mean().add_prefix('doc_conv_mean2_').reset_index().fillna(0)\n",
    "        q5 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).min().add_prefix('doc_conv_min_').reset_index().fillna(0)\n",
    "        q6 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).min().add_prefix('doc_conv_min2_').reset_index().fillna(0)\n",
    "        q7 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).max().add_prefix('doc_conv_max_').reset_index().fillna(0)\n",
    "        q8 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).max().add_prefix('doc_conv_max2_').reset_index().fillna(0)\n",
    "        q9 = q.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).std().add_prefix('doc_conv_std_').reset_index().fillna(0)\n",
    "        q10 = q_.drop('PERIODO',axis=1).groupby(['nro_documento_hash']).std().add_prefix('doc_conv_std2_').reset_index().fillna(0)\n",
    "        \n",
    "        convergente_hist.append(partial.merge(q1,on='nro_documento_hash',how='left')\n",
    "                                .merge(q2,on='nro_documento_hash',how='left').merge(q3,on='nro_documento_hash',how='left')\n",
    "                                .merge(q4,on='nro_documento_hash',how='left').merge(q5,on='nro_documento_hash',how='left')\n",
    "                                .merge(q6,on='nro_documento_hash',how='left').merge(q7,on='nro_documento_hash',how='left')\n",
    "                                .merge(q8,on='nro_documento_hash',how='left').merge(q9,on='nro_documento_hash',how='left')\n",
    "                                .merge(q10,on='nro_documento_hash',how='left')\n",
    "                        )\n",
    "convergente_hist = pd.concat(convergente_hist,sort=False)\n",
    "convergente_hist=convergente_hist.drop_duplicates()\n",
    "convergente_hist['key_documento']=convergente_hist.PERIODO.astype(str)+'_'+convergente_hist.nro_documento_hash.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergente.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,convergente,on=['key_documento'],how='left')\n",
    "\n",
    "convergente_hist.drop(['PERIODO','nro_documento_hash'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,convergente_hist,on=['key_documento'],how='left')\n",
    "\n",
    "print(df_full_hist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roaming=roaming_202201_202204.drop_duplicates()\n",
    "roaming['key_cliente']=roaming.PERIODO.astype(str)+'_'+roaming.nro_telefono_hash.astype(str)\n",
    "roaming=pd.merge(roaming,df_full_hist[['key_cliente','nro_documento_hash']],on=['key_cliente'],how='left')\n",
    "roaming['key_documento']=roaming.PERIODO.astype(str)+'_'+roaming.nro_documento_hash.astype(str)\n",
    "roaming.drop(['nro_telefono_hash','key_cliente'],axis=1,inplace=True)\n",
    "roaming.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roaming['FECHATRAFICO'] = pd.to_datetime(roaming['FECHATRAFICO'] , format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roaming=roaming[roaming['FECHATRAFICO']>=pd.to_datetime('2022-01-01 00:00:00')]\n",
    "roaming['codmes_fec_traf']=roaming['FECHATRAFICO'].astype(str).str.slice(0,4)+roaming['FECHATRAFICO'].astype(str).str.slice(5,7)\n",
    "roaming.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roaming['TIPOSERVICIO'] = replace_low_classes(roaming['TIPOSERVICIO'],thresh=1000,string='TIPO2').values # 7 =7 a más"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "roaming_hist = []\n",
    "data_roaming=roaming[['key_documento','PERIODO','MINUTOS','GIGAS','MENSAJES']]\n",
    "for mes in tqdm(data_roaming['PERIODO'].unique()):\n",
    "        partial = data_roaming[data_roaming['PERIODO']==mes][['PERIODO','key_documento']] # verano de enero\n",
    "        q = data_roaming[(data_roaming['key_documento'].isin(partial['key_documento'].unique()))\n",
    "                     & (data_roaming['PERIODO']==mes)]\n",
    "        \n",
    "        q1 = q.drop('PERIODO',axis=1).groupby(['key_documento']).sum().add_prefix('key_sum_').reset_index().fillna(0)\n",
    "        q2 = q.drop('PERIODO',axis=1).groupby(['key_documento']).mean().add_prefix('key_prom_').reset_index().fillna(0)\n",
    "        q3 = q.drop('PERIODO',axis=1).groupby(['key_documento']).median().add_prefix('key_md_').reset_index().fillna(0)\n",
    "        roaming_hist.append(partial.merge(q1,on='key_documento',how='left')\n",
    "                                .merge(q2,on='key_documento',how='left').merge(q3,on='key_documento',how='left')\n",
    "                        )\n",
    "roaming_hist = pd.concat(roaming_hist,sort=False)\n",
    "roaming_hist=roaming_hist.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total de categorias (grupo total) usadas vs scort de Categoria a nivel de codigo persona\n",
    "roaming_hist2 = []\n",
    "for mes in tqdm(roaming['PERIODO'].unique()):\n",
    "        partial = roaming[roaming['PERIODO']==mes][['PERIODO','key_documento']] # verano de enero\n",
    "        q = roaming[(roaming['key_documento'].isin(partial['key_documento'].unique()))\n",
    "                           & (roaming.PERIODO==mes)]\n",
    "        \n",
    "        q1 = q.groupby(['key_documento','TIPOSERVICIO'])['MINUTOS'].sum().unstack().add_prefix('sum_min_').reset_index().fillna(0)\n",
    "        q2 = q.groupby(['key_documento','TIPOSERVICIO'])['MINUTOS'].mean().unstack().add_prefix('mean_min_').reset_index().fillna(0)\n",
    "        q3 = q.groupby(['key_documento','TIPOSERVICIO'])['GIGAS'].sum().unstack().add_prefix('sum_gigas_').reset_index().fillna(0)\n",
    "        q4 = q.groupby(['key_documento','TIPOSERVICIO'])['GIGAS'].mean().unstack().add_prefix('mean_gigas_').reset_index().fillna(0)\n",
    "        q5 = q.groupby(['key_documento','TIPOSERVICIO'])['MENSAJES'].sum().unstack().add_prefix('sum_sms_').reset_index().fillna(0)\n",
    "        q6 = q.groupby(['key_documento','TIPOSERVICIO'])['MENSAJES'].mean().unstack().add_prefix('mean_sms_').reset_index().fillna(0)\n",
    "\n",
    "        roaming_hist2.append(partial.merge(q1,on='key_documento',how='left')\n",
    "                           .merge(q2,on='key_documento',how='left').merge(q3,on='key_documento',how='left')\n",
    "                           .merge(q4,on='key_documento',how='left').merge(q5,on='key_documento',how='left')\n",
    "                             .merge(q6,on='key_documento',how='left')\n",
    "                        )\n",
    "roaming_hist2 = pd.concat(roaming_hist2,sort=False)\n",
    "roaming_hist2=roaming_hist2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roaming_hist.drop(['PERIODO'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,roaming_hist,on=['key_documento'],how='left')\n",
    "df_full_hist.shape\n",
    "\n",
    "roaming_hist2.drop(['PERIODO'],axis=1,inplace=True)\n",
    "df_full_hist=pd.merge(df_full_hist,roaming_hist2,on=['key_documento'],how='left')\n",
    "df_full_hist.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INGRESO DE VARIABLES AL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_full_hist.to_csv('df_full_hist_2.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_full_hist=pd.read_csv(file_path+'df_full_hist_2.csv')\n",
    "df_full_hist_2=pd.read_csv(file_path+'df_full_hist.csv')\n",
    "perfil_digital=pd.read_csv(file_path+'perfil_digital.csv')\n",
    "suscriptora=pd.read_csv(file_path+'suscriptora.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_full_hist.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "tmp=df_full_hist_2[['key_cliente','marca_counts','modelo_counts','ngama_counts'\n",
    "                                        ,'os_counts','device_counts','rel_use','rel_use_full']]\n",
    "df_full_hist=pd.merge(df_full_hist,tmp,on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,perfil_digital[['key_cliente','scorecat_counts']],on=['key_cliente'],how='left')\n",
    "df_full_hist=pd.merge(df_full_hist,suscriptora[['key_cliente','tipo_adq_counts']],on=['key_cliente'],how='left')\n",
    "del df_full_hist_2,perfil_digital,suscriptora\n",
    "df_full_hist.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_full_hist.copy()\n",
    "df.drop(['NUMPERIODO','nro_documento_hash', 'nro_telefono_hash', 'key_documento'],axis=1,inplace=True)\n",
    "categorical=[\n",
    "# suscriptora\n",
    "        'TIPO_ADQ'\n",
    "    #terminales\n",
    "        #,'MARCA','MODELO','NUEVA_GAMMA','OS','DEVICE_TYPE'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datos_imputados(variable):\n",
    "    # buscamos variables categoricas y discretas\n",
    "    temp = df.groupby([variable])[variable].count()/np.float64(len(df))\n",
    "    frecuencia_cat = [x for x in temp.loc[temp>0.01].index.values]\n",
    "    \n",
    "    df[variable] = np.where(df[variable].isin(frecuencia_cat), df[variable], 'Raro')\n",
    "\n",
    "for var in categorical:\n",
    "    datos_imputados(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codificar_var_categoricas(var, target):\n",
    "        # tomamos como referencia el target para poder codificar\n",
    "        ordenamos_cat = df.groupby([var])[target].sum().sort_values().index # ordenamos en funcion a peso del target\n",
    "        transf_cat = {k:i for i, k in enumerate(ordenamos_cat, 0)} # transformamos a continuas\n",
    "        \n",
    "        # codificamos el set\n",
    "        df[var] = df[var].map(transf_cat)\n",
    "\n",
    "# codificamos las variables\n",
    "for var in categorical:\n",
    "    codificar_var_categoricas(var, 'TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forw=df.copy()\n",
    "df_forw['clase'] = 0\n",
    "df_forw.loc[df_forw.TARGET.notnull(),'clase']='train' #training\n",
    "df_forw.loc[df_forw.TARGET.isnull(),'clase']='test' #back_test\n",
    "train=df_forw.loc[df_forw.clase=='train'].set_index('key_cliente').drop(['clase','TARGET'],axis=1)\n",
    "test=df_forw.loc[df_forw.clase=='test'].set_index('key_cliente').drop(['clase','TARGET'],axis=1)\n",
    "\n",
    "y_train=df_forw.loc[df_forw.clase=='train'].set_index('key_cliente').TARGET.astype(int)  ## cambiamos el tipo de target\n",
    "y_test=df_forw.loc[df_forw.clase=='test'].set_index('key_cliente').TARGET ## cambiamos el tipo de target\n",
    "# limpiamos nommbres y caracteres extraños de nombres de columnas\n",
    "test = test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_-]+', '', x))\n",
    "train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_-]+', '', x))\n",
    "print('Training',train.shape,'Testing',test.shape) ###(358487, 2630) (396666, 2630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos valores unicos\n",
    "\n",
    "cols_with_onlyone_val = train.columns[train.nunique() == 1]\n",
    "print(cols_with_onlyone_val)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\n",
    "test.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "NUM_OF_DECIMALS = 32\n",
    "train = train.round(NUM_OF_DECIMALS)\n",
    "test = test.round(NUM_OF_DECIMALS)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToRemove = []\n",
    "columns = train.columns\n",
    "for i in range(len(columns)-1):\n",
    "    v = train[columns[i]].values\n",
    "    dupCols = []\n",
    "    for j in range(i + 1,len(columns)):\n",
    "        if np.array_equal(v, train[columns[j]].values):\n",
    "            colsToRemove.append(columns[j])\n",
    "train.drop(colsToRemove, axis=1, inplace=True) \n",
    "test.drop(colsToRemove, axis=1, inplace=True) \n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_train = missing_values_table(train)\n",
    "missing_train_vars = list(missing_train.index[missing_train['% de valores totales'] > 90])\n",
    "missing_test = missing_values_table(test)\n",
    "missing_test_vars = list(missing_test.index[missing_test['% de valores totales'] > 90])\n",
    "missing_columns = list(set(missing_test_vars + missing_train_vars))\n",
    "# Drop the missing columns\n",
    "train = train.drop(columns = missing_columns)\n",
    "test = test.drop(columns = missing_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import random_projection\n",
    "\n",
    "weight = ((train != 0).sum()/len(train)).values\n",
    "tmp_train = train[train!=0]\n",
    "tmp_test = test[test!=0]\n",
    "tmp = pd.concat([train,test])#RandomProjection\n",
    "ntrain = len(train)\n",
    "ntest = len(test)\n",
    "\n",
    "train[\"weight_count\"] = (tmp_train*weight).sum(axis=1)\n",
    "test[\"weight_count\"] = (tmp_test*weight).sum(axis=1)\n",
    "\n",
    "train[\"count_not0\"] = (train != 0).sum(axis=1)\n",
    "test[\"count_not0\"] = (test != 0).sum(axis=1)\n",
    "\n",
    "train[\"sum\"] = train.sum(axis=1)\n",
    "test[\"sum\"] = test.sum(axis=1)\n",
    "\n",
    "train[\"var\"] = tmp_train.var(axis=1)\n",
    "test[\"var\"] = tmp_test.var(axis=1)\n",
    "\n",
    "train[\"mean\"] = tmp_train.mean(axis=1)\n",
    "test[\"mean\"] = tmp_test.mean(axis=1)\n",
    "\n",
    "train[\"std\"] = tmp_train.std(axis=1)\n",
    "test[\"std\"] = tmp_test.std(axis=1)\n",
    "\n",
    "train[\"max\"] = tmp_train.max(axis=1)\n",
    "test[\"max\"] = tmp_test.max(axis=1)\n",
    "\n",
    "train[\"min\"] = tmp_train.min(axis=1)\n",
    "test[\"min\"] = tmp_test.min(axis=1)\n",
    "#######################\n",
    "#train[\"median\"] = tmp_train.median(axis=1)\n",
    "#test[\"median\"] = tmp_test.median(axis=1)\n",
    "\n",
    "#train[\"skew\"] = tmp_train.skew(axis=1)\n",
    "#test[\"skew\"] = tmp_test.skew(axis=1)\n",
    "\n",
    "#train[\"kurtosis\"] = tmp_train.kurtosis(axis=1)\n",
    "#test[\"kurtosis\"] = tmp_test.kurtosis(axis=1)\n",
    "\n",
    "del(tmp_train)\n",
    "del(tmp_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.stats import ks_2samp\n",
    "THRESHOLD_P_VALUE = 0.01 #need tuned\n",
    "THRESHOLD_STATISTIC = 0.3 #need tuned\n",
    "diff_cols = []\n",
    "for col in train.columns:\n",
    "    statistic, pvalue = ks_2samp(train[col].values, test[col].values)\n",
    "    if pvalue <= THRESHOLD_P_VALUE and np.abs(statistic) > THRESHOLD_STATISTIC:\n",
    "        diff_cols.append(col)\n",
    "for col in diff_cols:\n",
    "    if col in train.columns:\n",
    "        train.drop(col, axis=1, inplace=True)\n",
    "        test.drop(col, axis=1, inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test['full_count_null'] = test.apply(lambda x: x.count(), axis=1)\n",
    "train['full_count_null'] = train.apply(lambda x: x.count(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training',train.shape,'Testing',test.shape) ###(358487, 2630) (396666, 2630)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_child_samples  : Número mínimo de datos necesarios en un niño (hoja). Según los documentos de LightGBM, este es un parámetro muy importante para evitar el sobreajuste.\n",
    "\n",
    "#### num_leaves (LightGBM): máximo de hojas de árboles para los estudiantes básicos. Un valor más alto da como resultado árboles más profundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modelo de lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs = []\n",
    "train_probs = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "train_probs = pd.concat(train_probs)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs1 = []\n",
    "train_probs1 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= 8 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=60 #100\n",
    "                            ,num_leaves=20  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs1.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs1.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs1 = pd.concat(test_probs1, axis=1).mean(axis=1)\n",
    "train_probs1 = pd.concat(train_probs1)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs1.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs2 = []\n",
    "train_probs2 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=50 #100\n",
    "                            ,num_leaves=20  #20\n",
    "                            #,reg_alpha=0 # 0\n",
    "                            #,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs2.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs2.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs2 = pd.concat(test_probs2, axis=1).mean(axis=1)\n",
    "train_probs2 = pd.concat(train_probs2)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs2.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs3 = []\n",
    "train_probs3 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            ,learning_rate=0.05\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs3.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs3.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs3 = pd.concat(test_probs3, axis=1).mean(axis=1)\n",
    "train_probs3 = pd.concat(train_probs3)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs3.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs4 = []\n",
    "train_probs4 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            ,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs4.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs4.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs4 = pd.concat(test_probs4, axis=1).mean(axis=1)\n",
    "train_probs4 = pd.concat(train_probs4)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs4.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs5 = []\n",
    "train_probs5 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= 5 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            ,learning_rate=0.05\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs5.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs5.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs5 = pd.concat(test_probs5, axis=1).mean(axis=1)\n",
    "train_probs5 = pd.concat(train_probs5)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs5.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs6 = []\n",
    "train_probs6 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            ,max_bin = 8\n",
    "                            ,min_child_samples=50 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=20, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs6.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs6.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs6 = pd.concat(test_probs6, axis=1).mean(axis=1)\n",
    "train_probs6 = pd.concat(train_probs6)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs6.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_envio=test_probs6.copy()\n",
    "test_envio.name = \"TARGET\"\n",
    "test_envio=pd.DataFrame(test_envio).reset_index()\n",
    "#test_envio['nro_telefono_hash']=0\n",
    "test_envio['nro_telefono_hash']=test_envio.key_cliente.str.slice(7,500)\n",
    "#test_envio=pd.merge(test_envio,df_full_hist[['key_cliente','nro_telefono_hash']],on=['key_cliente'],how='left')\n",
    "test_envio.drop('key_cliente', axis=1,inplace=True)\n",
    "summission=test_envio[['nro_telefono_hash','TARGET']].set_index('nro_telefono_hash')\n",
    "summission.to_csv('summission_T6.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos el algoritms para poder entrenar y predecir\n",
    "gc.collect()\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs7 = []\n",
    "train_probs7 = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index]\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index]\n",
    "    \n",
    "    learner = LGBMClassifier(\n",
    "                            n_estimators=10000\n",
    "                            ,importance_type='gain'\n",
    "                            #,class_weight='balanced'\n",
    "                            ,max_depth= -1 # -1\n",
    "                            #,max_bin = 8\n",
    "                            #,min_child_samples=100 #100\n",
    "                            ,num_leaves=10  #20\n",
    "                            ,reg_alpha=0 # 0\n",
    "                            ,reg_lambda=1 # 1\n",
    "                            #,learning_rate=0.1\n",
    "                                )\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50) #25 despues  del tuning\n",
    "    test_probs7.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs7.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "          \n",
    "test_probs7 = pd.concat(test_probs7, axis=1).mean(axis=1)\n",
    "train_probs7 = pd.concat(train_probs7)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs7.loc[y_train.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emsamble promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembled_probs1=(test_probs + test_probs1 + test_probs2 + test_probs3 +test_probs4+test_probs5+ test_probs6 + test_probs7 )/8\n",
    "#print(test_probs_apv.shape,test_probs.shape)\n",
    "\n",
    "test_envio=ensembled_probs1.copy()\n",
    "test_envio.name = \"TARGET\"\n",
    "test_envio=pd.DataFrame(test_envio).reset_index()\n",
    "#test_envio['nro_telefono_hash']=0\n",
    "test_envio['nro_telefono_hash']=test_envio.key_cliente.str.slice(7,500)\n",
    "#test_envio=pd.merge(test_envio,df_full_hist[['key_cliente','nro_telefono_hash']],on=['key_cliente'],how='left')\n",
    "test_envio.drop('key_cliente', axis=1,inplace=True)\n",
    "summission=test_envio[['nro_telefono_hash','TARGET']].set_index('nro_telefono_hash')\n",
    "summission.to_csv('summission_lgbms_prom.csv',sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
